{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import applications, optimizers\n",
    "from keras.layers import Dropout, Flatten, Dense, Input\n",
    "from keras.models import Sequential, Model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensions of our images.\n",
    "img_width, img_height = 224, 224\n",
    "\n",
    "train_data_dir = '../dataset/train'\n",
    "test_data_dir = '../dataset/test'\n",
    "validation_data_dir = '../dataset/validation'\n",
    "nb_train_samples = 1792\n",
    "nb_test_samples = 210\n",
    "nb_validation_samples = 525"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildModel():\n",
    "    # build the ResNET50 network\n",
    "    base_model = applications.ResNet50(weights='imagenet', \n",
    "                                    include_top=False,\n",
    "                                    input_tensor=Input(shape=(img_width, img_height, 3)))\n",
    "\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "        \n",
    "    top_model = base_model.output\n",
    "    top_model = Flatten(name=\"Flatten\")(top_model)\n",
    "    top_model = Dense(512, activation='relu')(top_model)\n",
    "    top_model = Dense(256, activation='relu')(top_model)\n",
    "    top_model = Dense(6, activation='softmax')(top_model)\n",
    "    \n",
    "    model = Model(inputs=base_model.input, outputs=top_model)\n",
    "\n",
    "    # model.summary()\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateData(batch_size):\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        horizontal_flip=True,\n",
    "        rescale=1. /255,\n",
    "        rotation_range=30,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2)\n",
    "\n",
    "    validation_datagen = ImageDataGenerator(rescale=1. /255)\n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        class_mode='categorical')\n",
    "\n",
    "    validation_generator = validation_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')\n",
    "    \n",
    "    return [train_generator,validation_generator]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fineTuneModel(model, train_generator, validation_generator, epochs, batch_size):\n",
    "    model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=nb_train_samples // batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=nb_validation_samples // batch_size)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMetrics(model, batch_size):\n",
    "    \n",
    "    test_datagen = ImageDataGenerator(rescale=1. /255)\n",
    "\n",
    "    test_generator = test_datagen.flow_from_directory(\n",
    "        test_data_dir,\n",
    "        target_size=(img_width,img_height),\n",
    "        shuffle=False,\n",
    "        batch_size=1,\n",
    "        class_mode='categorical')\n",
    "    \n",
    "    predicted_results = model.predict_generator(test_generator, \n",
    "                                                steps = nb_test_samples)\n",
    "    predicted_results = np.argmax(predicted_results, axis=1)\n",
    "    targets = [\"cardboard\", \"glass\", \"metal\", \"paper\", \"plastic\", \"trash\"]\n",
    "    \n",
    "    # confusion matrix\n",
    "    print(\"CONFUSION MATRIX:\")\n",
    "    print(confusion_matrix(test_generator.classes, predicted_results))\n",
    "    \n",
    "    # classification report\n",
    "    print(\"CLASSIFICATION REPORT:\")\n",
    "    print(classification_report(test_generator.classes, predicted_results, target_names=targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runExperiments():\n",
    "    batch_sizes = [8,16,32]\n",
    "    epochs = [10,20,30]\n",
    "    config = 1\n",
    "    \n",
    "    for batch_size in batch_sizes:\n",
    "        for epoch in epochs:\n",
    "            \n",
    "            print(\"*************** Test \", config, \" Batch size: \", batch_size, \" Epochs: \", epoch, \"***************\")           \n",
    "            keras.backend.clear_session()\n",
    "            \n",
    "            model = buildModel()\n",
    "            train_generator,test_generator = generateData(batch_size)\n",
    "            trained_model = fineTuneModel(model,train_generator, test_generator, epoch, batch_size)\n",
    "            metrics = getMetrics(trained_model, batch_size)\n",
    "            \n",
    "            config += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************** Test  1  Batch size:  8  Epochs:  10 ***************\n",
      "Found 1792 images belonging to 6 classes.\n",
      "Found 525 images belonging to 6 classes.\n",
      "Epoch 1/10\n",
      "224/224 [==============================] - 585s 3s/step - loss: 12.8501 - acc: 0.1998 - val_loss: 13.0185 - val_acc: 0.1923\n",
      "Epoch 2/10\n",
      "224/224 [==============================] - 560s 3s/step - loss: 12.8801 - acc: 0.2009 - val_loss: 13.0628 - val_acc: 0.1896\n",
      "Epoch 3/10\n",
      "224/224 [==============================] - 561s 3s/step - loss: 12.8801 - acc: 0.2009 - val_loss: 12.8446 - val_acc: 0.2031\n",
      "Epoch 4/10\n",
      "224/224 [==============================] - 561s 3s/step - loss: 12.8801 - acc: 0.2009 - val_loss: 13.0628 - val_acc: 0.1896\n",
      "Epoch 5/10\n",
      "224/224 [==============================] - 557s 2s/step - loss: 12.8801 - acc: 0.2009 - val_loss: 13.0940 - val_acc: 0.1876\n",
      "Epoch 6/10\n",
      "224/224 [==============================] - 555s 2s/step - loss: 12.8801 - acc: 0.2009 - val_loss: 12.9381 - val_acc: 0.1973\n",
      "Epoch 7/10\n",
      "224/224 [==============================] - 557s 2s/step - loss: 12.8801 - acc: 0.2009 - val_loss: 13.0317 - val_acc: 0.1915\n",
      "Epoch 8/10\n",
      "224/224 [==============================] - 558s 2s/step - loss: 12.8801 - acc: 0.2009 - val_loss: 13.1252 - val_acc: 0.1857\n",
      "Epoch 9/10\n",
      "224/224 [==============================] - 557s 2s/step - loss: 12.8801 - acc: 0.2009 - val_loss: 13.1564 - val_acc: 0.1838\n",
      "Epoch 10/10\n",
      "224/224 [==============================] - 555s 2s/step - loss: 12.8801 - acc: 0.2009 - val_loss: 12.8446 - val_acc: 0.2031\n",
      "Found 210 images belonging to 6 classes.\n",
      "CONFUSION MATRIX:\n",
      "[[ 0 40  0  0  0  0]\n",
      " [ 0 40  0  0  0  0]\n",
      " [ 0 40  0  0  0  0]\n",
      " [ 0 40  0  0  0  0]\n",
      " [ 0 40  0  0  0  0]\n",
      " [ 0 10  0  0  0  0]]\n",
      "CLASSIFICATION REPORT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   cardboard       0.00      0.00      0.00        40\n",
      "       glass       0.19      1.00      0.32        40\n",
      "       metal       0.00      0.00      0.00        40\n",
      "       paper       0.00      0.00      0.00        40\n",
      "     plastic       0.00      0.00      0.00        40\n",
      "       trash       0.00      0.00      0.00        10\n",
      "\n",
      "   micro avg       0.19      0.19      0.19       210\n",
      "   macro avg       0.03      0.17      0.05       210\n",
      "weighted avg       0.04      0.19      0.06       210\n",
      "\n",
      "*************** Test  2  Batch size:  8  Epochs:  20 ***************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martin/anaconda3/envs/martin/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1792 images belonging to 6 classes.\n",
      "Found 525 images belonging to 6 classes.\n",
      "Epoch 1/20\n",
      "224/224 [==============================] - 586s 3s/step - loss: 11.9940 - acc: 0.2517 - val_loss: 13.0495 - val_acc: 0.1904\n",
      "Epoch 2/20\n",
      "224/224 [==============================] - 563s 3s/step - loss: 12.0346 - acc: 0.2533 - val_loss: 13.0005 - val_acc: 0.1934\n",
      "Epoch 3/20\n",
      "224/224 [==============================] - 563s 3s/step - loss: 12.0346 - acc: 0.2533 - val_loss: 13.1875 - val_acc: 0.1818\n",
      "Epoch 4/20\n",
      "224/224 [==============================] - 563s 3s/step - loss: 12.0346 - acc: 0.2533 - val_loss: 12.9693 - val_acc: 0.1954\n",
      "Epoch 5/20\n",
      "224/224 [==============================] - 564s 3s/step - loss: 12.0346 - acc: 0.2533 - val_loss: 13.1252 - val_acc: 0.1857\n",
      "Epoch 6/20\n",
      "224/224 [==============================] - 561s 3s/step - loss: 12.0346 - acc: 0.2533 - val_loss: 13.0005 - val_acc: 0.1934\n",
      "Epoch 7/20\n",
      "224/224 [==============================] - 562s 3s/step - loss: 12.0346 - acc: 0.2533 - val_loss: 13.0317 - val_acc: 0.1915\n",
      "Epoch 8/20\n",
      "224/224 [==============================] - 559s 2s/step - loss: 12.0346 - acc: 0.2533 - val_loss: 13.0317 - val_acc: 0.1915\n",
      "Epoch 9/20\n",
      "224/224 [==============================] - 559s 2s/step - loss: 12.0346 - acc: 0.2533 - val_loss: 12.9693 - val_acc: 0.1954\n",
      "Epoch 10/20\n",
      "224/224 [==============================] - 559s 2s/step - loss: 12.0346 - acc: 0.2533 - val_loss: 13.0628 - val_acc: 0.1896\n",
      "Epoch 11/20\n",
      "224/224 [==============================] - 559s 2s/step - loss: 12.0346 - acc: 0.2533 - val_loss: 13.0005 - val_acc: 0.1934\n",
      "Epoch 12/20\n",
      "224/224 [==============================] - 559s 2s/step - loss: 12.0346 - acc: 0.2533 - val_loss: 13.0005 - val_acc: 0.1934\n",
      "Epoch 13/20\n",
      "224/224 [==============================] - 559s 2s/step - loss: 12.0346 - acc: 0.2533 - val_loss: 13.3434 - val_acc: 0.1721\n",
      "Epoch 14/20\n",
      "224/224 [==============================] - 560s 2s/step - loss: 12.0346 - acc: 0.2533 - val_loss: 12.9069 - val_acc: 0.1992\n",
      "Epoch 15/20\n",
      "224/224 [==============================] - 558s 2s/step - loss: 12.0346 - acc: 0.2533 - val_loss: 13.1252 - val_acc: 0.1857\n",
      "Epoch 16/20\n",
      "224/224 [==============================] - 559s 2s/step - loss: 12.0346 - acc: 0.2533 - val_loss: 13.0317 - val_acc: 0.1915\n",
      "Epoch 17/20\n",
      "224/224 [==============================] - 558s 2s/step - loss: 12.0346 - acc: 0.2533 - val_loss: 12.8446 - val_acc: 0.2031\n",
      "Epoch 18/20\n",
      "224/224 [==============================] - 559s 2s/step - loss: 12.0346 - acc: 0.2533 - val_loss: 13.0940 - val_acc: 0.1876\n",
      "Epoch 19/20\n",
      "224/224 [==============================] - 556s 2s/step - loss: 12.0346 - acc: 0.2533 - val_loss: 13.2187 - val_acc: 0.1799\n",
      "Epoch 20/20\n",
      "224/224 [==============================] - 552s 2s/step - loss: 12.0346 - acc: 0.2533 - val_loss: 13.0317 - val_acc: 0.1915\n",
      "Found 210 images belonging to 6 classes.\n",
      "CONFUSION MATRIX:\n",
      "[[ 0  0  0 40  0  0]\n",
      " [ 0  0  0 40  0  0]\n",
      " [ 0  0  0 40  0  0]\n",
      " [ 0  0  0 40  0  0]\n",
      " [ 0  0  0 40  0  0]\n",
      " [ 0  0  0 10  0  0]]\n",
      "CLASSIFICATION REPORT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   cardboard       0.00      0.00      0.00        40\n",
      "       glass       0.00      0.00      0.00        40\n",
      "       metal       0.00      0.00      0.00        40\n",
      "       paper       0.19      1.00      0.32        40\n",
      "     plastic       0.00      0.00      0.00        40\n",
      "       trash       0.00      0.00      0.00        10\n",
      "\n",
      "   micro avg       0.19      0.19      0.19       210\n",
      "   macro avg       0.03      0.17      0.05       210\n",
      "weighted avg       0.04      0.19      0.06       210\n",
      "\n",
      "*************** Test  3  Batch size:  8  Epochs:  30 ***************\n",
      "Found 1792 images belonging to 6 classes.\n",
      "Found 525 images belonging to 6 classes.\n",
      "Epoch 1/30\n",
      "224/224 [==============================] - 581s 3s/step - loss: 12.7304 - acc: 0.2065 - val_loss: 13.0185 - val_acc: 0.1923\n",
      "Epoch 2/30\n",
      "224/224 [==============================] - 554s 2s/step - loss: 12.8801 - acc: 0.2009 - val_loss: 13.0317 - val_acc: 0.1915\n",
      "Epoch 3/30\n",
      "224/224 [==============================] - 561s 3s/step - loss: 12.8801 - acc: 0.2009 - val_loss: 13.0940 - val_acc: 0.1876\n",
      "Epoch 4/30\n",
      "224/224 [==============================] - 559s 2s/step - loss: 12.8801 - acc: 0.2009 - val_loss: 12.9381 - val_acc: 0.1973\n",
      "Epoch 5/30\n",
      "224/224 [==============================] - 562s 3s/step - loss: 12.8801 - acc: 0.2009 - val_loss: 12.9693 - val_acc: 0.1954\n",
      "Epoch 6/30\n",
      "224/224 [==============================] - 559s 2s/step - loss: 12.8801 - acc: 0.2009 - val_loss: 13.0317 - val_acc: 0.1915\n",
      "Epoch 7/30\n",
      "224/224 [==============================] - 555s 2s/step - loss: 12.8801 - acc: 0.2009 - val_loss: 13.0317 - val_acc: 0.1915\n",
      "Epoch 8/30\n",
      "224/224 [==============================] - 556s 2s/step - loss: 12.8801 - acc: 0.2009 - val_loss: 13.0317 - val_acc: 0.1915\n",
      "Epoch 9/30\n",
      "224/224 [==============================] - 554s 2s/step - loss: 12.8801 - acc: 0.2009 - val_loss: 12.9381 - val_acc: 0.1973\n",
      "Epoch 10/30\n",
      "224/224 [==============================] - 555s 2s/step - loss: 12.8801 - acc: 0.2009 - val_loss: 13.1875 - val_acc: 0.1818\n",
      "Epoch 11/30\n",
      "224/224 [==============================] - 557s 2s/step - loss: 12.8801 - acc: 0.2009 - val_loss: 12.9069 - val_acc: 0.1992\n",
      "Epoch 12/30\n",
      "224/224 [==============================] - 554s 2s/step - loss: 12.8801 - acc: 0.2009 - val_loss: 12.8758 - val_acc: 0.2012\n",
      "Epoch 13/30\n",
      "224/224 [==============================] - 553s 2s/step - loss: 12.8801 - acc: 0.2009 - val_loss: 13.2811 - val_acc: 0.1760\n",
      "Epoch 14/30\n",
      "224/224 [==============================] - 554s 2s/step - loss: 12.8801 - acc: 0.2009 - val_loss: 12.8758 - val_acc: 0.2012\n",
      "Epoch 15/30\n",
      "224/224 [==============================] - 554s 2s/step - loss: 12.8711 - acc: 0.2015 - val_loss: 13.2811 - val_acc: 0.1760\n",
      "Epoch 16/30\n",
      "224/224 [==============================] - 555s 2s/step - loss: 12.8801 - acc: 0.2009 - val_loss: 12.6887 - val_acc: 0.2128\n",
      "Epoch 17/30\n",
      "224/224 [==============================] - 560s 2s/step - loss: 12.8801 - acc: 0.2009 - val_loss: 13.1875 - val_acc: 0.1818\n",
      "Epoch 18/30\n",
      "224/224 [==============================] - 559s 2s/step - loss: 12.8806 - acc: 0.2003 - val_loss: 13.2499 - val_acc: 0.1779\n",
      "Epoch 19/30\n",
      "224/224 [==============================] - 560s 2s/step - loss: 12.0346 - acc: 0.2533 - val_loss: 13.0317 - val_acc: 0.1915\n",
      "Epoch 20/30\n",
      "224/224 [==============================] - 559s 2s/step - loss: 12.0346 - acc: 0.2533 - val_loss: 13.0628 - val_acc: 0.1896\n",
      "Epoch 21/30\n",
      "224/224 [==============================] - 560s 3s/step - loss: 12.0346 - acc: 0.2533 - val_loss: 12.9693 - val_acc: 0.1954\n",
      "Epoch 22/30\n",
      "224/224 [==============================] - 561s 3s/step - loss: 12.0346 - acc: 0.2533 - val_loss: 13.1875 - val_acc: 0.1818\n",
      "Epoch 23/30\n",
      "224/224 [==============================] - 560s 2s/step - loss: 12.0346 - acc: 0.2533 - val_loss: 12.9069 - val_acc: 0.1992\n",
      "Epoch 24/30\n",
      "224/224 [==============================] - 560s 3s/step - loss: 12.0346 - acc: 0.2533 - val_loss: 13.0005 - val_acc: 0.1934\n",
      "Epoch 25/30\n",
      "224/224 [==============================] - 558s 2s/step - loss: 12.0346 - acc: 0.2533 - val_loss: 12.9069 - val_acc: 0.1992\n",
      "Epoch 26/30\n",
      "224/224 [==============================] - 562s 3s/step - loss: 12.0346 - acc: 0.2533 - val_loss: 13.2811 - val_acc: 0.1760\n",
      "Epoch 27/30\n",
      "224/224 [==============================] - 560s 3s/step - loss: 12.0346 - acc: 0.2533 - val_loss: 13.0628 - val_acc: 0.1896\n",
      "Epoch 28/30\n",
      "224/224 [==============================] - 560s 2s/step - loss: 12.0346 - acc: 0.2533 - val_loss: 12.9381 - val_acc: 0.1973\n",
      "Epoch 29/30\n",
      "224/224 [==============================] - 560s 2s/step - loss: 12.0346 - acc: 0.2533 - val_loss: 13.0005 - val_acc: 0.1934\n",
      "Epoch 30/30\n",
      "224/224 [==============================] - 554s 2s/step - loss: 12.0346 - acc: 0.2533 - val_loss: 12.8134 - val_acc: 0.2050\n",
      "Found 210 images belonging to 6 classes.\n",
      "CONFUSION MATRIX:\n",
      "[[ 0  0  0 40  0  0]\n",
      " [ 0  0  0 40  0  0]\n",
      " [ 0  0  0 40  0  0]\n",
      " [ 0  0  0 40  0  0]\n",
      " [ 0  0  0 40  0  0]\n",
      " [ 0  0  0 10  0  0]]\n",
      "CLASSIFICATION REPORT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   cardboard       0.00      0.00      0.00        40\n",
      "       glass       0.00      0.00      0.00        40\n",
      "       metal       0.00      0.00      0.00        40\n",
      "       paper       0.19      1.00      0.32        40\n",
      "     plastic       0.00      0.00      0.00        40\n",
      "       trash       0.00      0.00      0.00        10\n",
      "\n",
      "   micro avg       0.19      0.19      0.19       210\n",
      "   macro avg       0.03      0.17      0.05       210\n",
      "weighted avg       0.04      0.19      0.06       210\n",
      "\n",
      "*************** Test  4  Batch size:  16  Epochs:  10 ***************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1792 images belonging to 6 classes.\n",
      "Found 525 images belonging to 6 classes.\n",
      "Epoch 1/10\n",
      "112/112 [==============================] - 558s 5s/step - loss: 12.9717 - acc: 0.1881 - val_loss: 13.0330 - val_acc: 0.1914\n",
      "Epoch 2/10\n",
      "112/112 [==============================] - 514s 5s/step - loss: 13.0420 - acc: 0.1908 - val_loss: 12.9515 - val_acc: 0.1965\n",
      "Epoch 3/10\n",
      "112/112 [==============================] - 512s 5s/step - loss: 13.0420 - acc: 0.1908 - val_loss: 13.2048 - val_acc: 0.1807\n",
      "Epoch 4/10\n",
      "112/112 [==============================] - 512s 5s/step - loss: 13.0420 - acc: 0.1908 - val_loss: 12.9515 - val_acc: 0.1965\n",
      "Epoch 5/10\n",
      "112/112 [==============================] - 511s 5s/step - loss: 13.0420 - acc: 0.1908 - val_loss: 13.0781 - val_acc: 0.1886\n",
      "Epoch 6/10\n",
      "112/112 [==============================] - 510s 5s/step - loss: 13.0420 - acc: 0.1908 - val_loss: 13.2365 - val_acc: 0.1788\n",
      "Epoch 7/10\n",
      "112/112 [==============================] - 510s 5s/step - loss: 13.0420 - acc: 0.1908 - val_loss: 12.8881 - val_acc: 0.2004\n",
      "Epoch 8/10\n",
      "112/112 [==============================] - 508s 5s/step - loss: 13.0420 - acc: 0.1908 - val_loss: 12.9831 - val_acc: 0.1945\n",
      "Epoch 9/10\n",
      "112/112 [==============================] - 511s 5s/step - loss: 13.0420 - acc: 0.1908 - val_loss: 13.1098 - val_acc: 0.1866\n",
      "Epoch 10/10\n",
      "112/112 [==============================] - 508s 5s/step - loss: 13.0420 - acc: 0.1908 - val_loss: 12.9198 - val_acc: 0.1984\n",
      "Found 210 images belonging to 6 classes.\n",
      "CONFUSION MATRIX:\n",
      "[[ 0  0  0  0 40  0]\n",
      " [ 0  0  0  0 40  0]\n",
      " [ 0  0  0  0 40  0]\n",
      " [ 0  0  0  0 40  0]\n",
      " [ 0  0  0  0 40  0]\n",
      " [ 0  0  0  0 10  0]]\n",
      "CLASSIFICATION REPORT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   cardboard       0.00      0.00      0.00        40\n",
      "       glass       0.00      0.00      0.00        40\n",
      "       metal       0.00      0.00      0.00        40\n",
      "       paper       0.00      0.00      0.00        40\n",
      "     plastic       0.19      1.00      0.32        40\n",
      "       trash       0.00      0.00      0.00        10\n",
      "\n",
      "   micro avg       0.19      0.19      0.19       210\n",
      "   macro avg       0.03      0.17      0.05       210\n",
      "weighted avg       0.04      0.19      0.06       210\n",
      "\n",
      "*************** Test  5  Batch size:  16  Epochs:  20 ***************\n",
      "Found 1792 images belonging to 6 classes.\n",
      "Found 525 images belonging to 6 classes.\n",
      "Epoch 1/20\n",
      "112/112 [==============================] - 550s 5s/step - loss: 13.6740 - acc: 0.1451 - val_loss: 13.0330 - val_acc: 0.1914\n",
      "Epoch 2/20\n",
      "112/112 [==============================] - 505s 5s/step - loss: 13.7525 - acc: 0.1468 - val_loss: 13.1098 - val_acc: 0.1866\n",
      "Epoch 3/20\n",
      "112/112 [==============================] - 508s 5s/step - loss: 13.7525 - acc: 0.1468 - val_loss: 12.9515 - val_acc: 0.1965\n",
      "Epoch 4/20\n",
      "112/112 [==============================] - 505s 5s/step - loss: 13.7525 - acc: 0.1468 - val_loss: 13.2681 - val_acc: 0.1768\n",
      "Epoch 5/20\n",
      "112/112 [==============================] - 506s 5s/step - loss: 13.7525 - acc: 0.1468 - val_loss: 12.7298 - val_acc: 0.2102\n",
      "Epoch 6/20\n",
      "112/112 [==============================] - 504s 5s/step - loss: 13.7525 - acc: 0.1468 - val_loss: 13.2681 - val_acc: 0.1768\n",
      "Epoch 7/20\n",
      "112/112 [==============================] - 504s 5s/step - loss: 13.7525 - acc: 0.1468 - val_loss: 13.0781 - val_acc: 0.1886\n",
      "Epoch 8/20\n",
      "112/112 [==============================] - 503s 4s/step - loss: 13.7525 - acc: 0.1468 - val_loss: 12.9831 - val_acc: 0.1945\n",
      "Epoch 9/20\n",
      "112/112 [==============================] - 506s 5s/step - loss: 13.7525 - acc: 0.1468 - val_loss: 13.1415 - val_acc: 0.1847\n",
      "Epoch 10/20\n",
      "112/112 [==============================] - 505s 5s/step - loss: 13.7525 - acc: 0.1468 - val_loss: 12.8881 - val_acc: 0.2004\n",
      "Epoch 11/20\n",
      "112/112 [==============================] - 506s 5s/step - loss: 13.7525 - acc: 0.1468 - val_loss: 13.1415 - val_acc: 0.1847\n",
      "Epoch 12/20\n",
      "112/112 [==============================] - 503s 4s/step - loss: 13.7525 - acc: 0.1468 - val_loss: 12.8565 - val_acc: 0.2024\n",
      "Epoch 13/20\n",
      "112/112 [==============================] - 506s 5s/step - loss: 13.7525 - acc: 0.1468 - val_loss: 13.4898 - val_acc: 0.1631\n",
      "Epoch 14/20\n",
      "112/112 [==============================] - 506s 5s/step - loss: 13.7525 - acc: 0.1468 - val_loss: 12.7931 - val_acc: 0.2063\n",
      "Epoch 15/20\n",
      "112/112 [==============================] - 505s 5s/step - loss: 13.7525 - acc: 0.1468 - val_loss: 12.9515 - val_acc: 0.1965\n",
      "Epoch 16/20\n",
      "112/112 [==============================] - 506s 5s/step - loss: 13.7525 - acc: 0.1468 - val_loss: 12.8881 - val_acc: 0.2004\n",
      "Epoch 17/20\n",
      "112/112 [==============================] - 505s 5s/step - loss: 13.7525 - acc: 0.1468 - val_loss: 13.1415 - val_acc: 0.1847\n",
      "Epoch 18/20\n",
      "112/112 [==============================] - 504s 5s/step - loss: 13.7525 - acc: 0.1468 - val_loss: 13.3315 - val_acc: 0.1729\n",
      "Epoch 19/20\n",
      "112/112 [==============================] - 506s 5s/step - loss: 13.7525 - acc: 0.1468 - val_loss: 12.9198 - val_acc: 0.1984\n",
      "Epoch 20/20\n",
      "112/112 [==============================] - 507s 5s/step - loss: 13.7525 - acc: 0.1468 - val_loss: 12.9515 - val_acc: 0.1965\n",
      "Found 210 images belonging to 6 classes.\n",
      "CONFUSION MATRIX:\n",
      "[[40  0  0  0  0  0]\n",
      " [40  0  0  0  0  0]\n",
      " [40  0  0  0  0  0]\n",
      " [40  0  0  0  0  0]\n",
      " [40  0  0  0  0  0]\n",
      " [10  0  0  0  0  0]]\n",
      "CLASSIFICATION REPORT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   cardboard       0.19      1.00      0.32        40\n",
      "       glass       0.00      0.00      0.00        40\n",
      "       metal       0.00      0.00      0.00        40\n",
      "       paper       0.00      0.00      0.00        40\n",
      "     plastic       0.00      0.00      0.00        40\n",
      "       trash       0.00      0.00      0.00        10\n",
      "\n",
      "   micro avg       0.19      0.19      0.19       210\n",
      "   macro avg       0.03      0.17      0.05       210\n",
      "weighted avg       0.04      0.19      0.06       210\n",
      "\n",
      "*************** Test  6  Batch size:  16  Epochs:  30 ***************\n",
      "Found 1792 images belonging to 6 classes.\n",
      "Found 525 images belonging to 6 classes.\n",
      "Epoch 1/30\n",
      "112/112 [==============================] - 556s 5s/step - loss: 11.9376 - acc: 0.2539 - val_loss: 13.0645 - val_acc: 0.1895\n",
      "Epoch 2/30\n",
      "112/112 [==============================] - 504s 4s/step - loss: 12.0256 - acc: 0.2539 - val_loss: 13.2048 - val_acc: 0.1807\n",
      "Epoch 3/30\n",
      "112/112 [==============================] - 506s 5s/step - loss: 12.0346 - acc: 0.2533 - val_loss: 12.8565 - val_acc: 0.2024\n",
      "Epoch 4/30\n",
      "112/112 [==============================] - 505s 5s/step - loss: 12.6734 - acc: 0.2137 - val_loss: 13.1415 - val_acc: 0.1847\n",
      "Epoch 5/30\n",
      "112/112 [==============================] - 506s 5s/step - loss: 13.7525 - acc: 0.1468 - val_loss: 12.9831 - val_acc: 0.1945\n",
      "Epoch 6/30\n",
      "112/112 [==============================] - 502s 4s/step - loss: 13.7525 - acc: 0.1468 - val_loss: 13.0781 - val_acc: 0.1886\n",
      "Epoch 7/30\n",
      "112/112 [==============================] - 506s 5s/step - loss: 13.7525 - acc: 0.1468 - val_loss: 13.0148 - val_acc: 0.1925\n",
      "Epoch 8/30\n",
      "112/112 [==============================] - 504s 5s/step - loss: 13.7525 - acc: 0.1468 - val_loss: 13.0781 - val_acc: 0.1886\n",
      "Epoch 9/30\n",
      "112/112 [==============================] - 506s 5s/step - loss: 13.7525 - acc: 0.1468 - val_loss: 13.2681 - val_acc: 0.1768\n",
      "Epoch 10/30\n",
      "112/112 [==============================] - 503s 4s/step - loss: 13.7525 - acc: 0.1468 - val_loss: 12.9198 - val_acc: 0.1984\n",
      "Epoch 11/30\n",
      "112/112 [==============================] - 505s 5s/step - loss: 13.7525 - acc: 0.1468 - val_loss: 13.0465 - val_acc: 0.1906\n",
      "Epoch 12/30\n",
      "112/112 [==============================] - 505s 5s/step - loss: 13.7525 - acc: 0.1468 - val_loss: 12.9831 - val_acc: 0.1945\n",
      "Epoch 13/30\n",
      "112/112 [==============================] - 504s 5s/step - loss: 13.7525 - acc: 0.1468 - val_loss: 13.2048 - val_acc: 0.1807\n",
      "Epoch 14/30\n",
      "112/112 [==============================] - 504s 5s/step - loss: 13.7525 - acc: 0.1468 - val_loss: 12.7615 - val_acc: 0.2083\n",
      "Epoch 15/30\n",
      "112/112 [==============================] - 504s 4s/step - loss: 13.7525 - acc: 0.1468 - val_loss: 13.1098 - val_acc: 0.1866\n",
      "Epoch 16/30\n",
      "112/112 [==============================] - 504s 5s/step - loss: 13.7525 - acc: 0.1468 - val_loss: 13.1731 - val_acc: 0.1827\n",
      "Epoch 17/30\n",
      "112/112 [==============================] - 502s 4s/step - loss: 13.7525 - acc: 0.1468 - val_loss: 13.0465 - val_acc: 0.1906\n",
      "Epoch 18/30\n",
      "112/112 [==============================] - 502s 4s/step - loss: 13.7525 - acc: 0.1468 - val_loss: 12.7931 - val_acc: 0.2063\n",
      "Epoch 19/30\n",
      "112/112 [==============================] - 504s 5s/step - loss: 13.7525 - acc: 0.1468 - val_loss: 13.0148 - val_acc: 0.1925\n",
      "Epoch 20/30\n",
      "112/112 [==============================] - 503s 4s/step - loss: 13.7525 - acc: 0.1468 - val_loss: 13.1098 - val_acc: 0.1866\n",
      "Epoch 21/30\n",
      "112/112 [==============================] - 508s 5s/step - loss: 13.7525 - acc: 0.1468 - val_loss: 13.2681 - val_acc: 0.1768\n",
      "Epoch 22/30\n",
      "112/112 [==============================] - 503s 4s/step - loss: 13.7525 - acc: 0.1468 - val_loss: 13.0465 - val_acc: 0.1906\n",
      "Epoch 23/30\n",
      "112/112 [==============================] - 502s 4s/step - loss: 13.7525 - acc: 0.1468 - val_loss: 12.7931 - val_acc: 0.2063\n",
      "Epoch 24/30\n",
      "112/112 [==============================] - 504s 5s/step - loss: 13.7525 - acc: 0.1468 - val_loss: 13.2365 - val_acc: 0.1788\n",
      "Epoch 25/30\n",
      "112/112 [==============================] - 510s 5s/step - loss: 13.7525 - acc: 0.1468 - val_loss: 13.0148 - val_acc: 0.1925\n",
      "Epoch 26/30\n",
      "112/112 [==============================] - 522s 5s/step - loss: 13.7525 - acc: 0.1468 - val_loss: 13.0465 - val_acc: 0.1906\n",
      "Epoch 27/30\n",
      "112/112 [==============================] - 504s 4s/step - loss: 13.7525 - acc: 0.1468 - val_loss: 13.1731 - val_acc: 0.1827\n",
      "Epoch 28/30\n",
      "112/112 [==============================] - 504s 5s/step - loss: 13.7525 - acc: 0.1468 - val_loss: 13.0148 - val_acc: 0.1925\n",
      "Epoch 29/30\n",
      "112/112 [==============================] - 504s 5s/step - loss: 13.7525 - acc: 0.1468 - val_loss: 12.8881 - val_acc: 0.2004\n",
      "Epoch 30/30\n",
      "112/112 [==============================] - 503s 4s/step - loss: 13.7525 - acc: 0.1468 - val_loss: 13.1415 - val_acc: 0.1847\n",
      "Found 210 images belonging to 6 classes.\n",
      "CONFUSION MATRIX:\n",
      "[[40  0  0  0  0  0]\n",
      " [40  0  0  0  0  0]\n",
      " [40  0  0  0  0  0]\n",
      " [40  0  0  0  0  0]\n",
      " [40  0  0  0  0  0]\n",
      " [10  0  0  0  0  0]]\n",
      "CLASSIFICATION REPORT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   cardboard       0.19      1.00      0.32        40\n",
      "       glass       0.00      0.00      0.00        40\n",
      "       metal       0.00      0.00      0.00        40\n",
      "       paper       0.00      0.00      0.00        40\n",
      "     plastic       0.00      0.00      0.00        40\n",
      "       trash       0.00      0.00      0.00        10\n",
      "\n",
      "   micro avg       0.19      0.19      0.19       210\n",
      "   macro avg       0.03      0.17      0.05       210\n",
      "weighted avg       0.04      0.19      0.06       210\n",
      "\n",
      "*************** Test  7  Batch size:  32  Epochs:  10 ***************\n",
      "Found 1792 images belonging to 6 classes.\n",
      "Found 525 images belonging to 6 classes.\n",
      "Epoch 1/10\n",
      "56/56 [==============================] - 517s 9s/step - loss: 12.9006 - acc: 0.1864 - val_loss: 13.0015 - val_acc: 0.1934\n",
      "Epoch 2/10\n",
      "56/56 [==============================] - 468s 8s/step - loss: 13.0420 - acc: 0.1908 - val_loss: 13.1103 - val_acc: 0.1866\n",
      "Epoch 3/10\n",
      "56/56 [==============================] - 468s 8s/step - loss: 13.0420 - acc: 0.1908 - val_loss: 12.8814 - val_acc: 0.2008\n",
      "Epoch 4/10\n",
      "56/56 [==============================] - 471s 8s/step - loss: 13.0420 - acc: 0.1908 - val_loss: 13.0776 - val_acc: 0.1886\n",
      "Epoch 5/10\n",
      "56/56 [==============================] - 468s 8s/step - loss: 13.0420 - acc: 0.1908 - val_loss: 13.0776 - val_acc: 0.1886\n",
      "Epoch 6/10\n",
      "56/56 [==============================] - 470s 8s/step - loss: 13.0420 - acc: 0.1908 - val_loss: 13.2737 - val_acc: 0.1765\n",
      "Epoch 7/10\n",
      "56/56 [==============================] - 471s 8s/step - loss: 13.0420 - acc: 0.1908 - val_loss: 13.3064 - val_acc: 0.1744\n",
      "Epoch 8/10\n",
      "56/56 [==============================] - 467s 8s/step - loss: 13.0420 - acc: 0.1908 - val_loss: 12.8814 - val_acc: 0.2008\n",
      "Epoch 9/10\n",
      "56/56 [==============================] - 480s 9s/step - loss: 13.0420 - acc: 0.1908 - val_loss: 12.7833 - val_acc: 0.2069\n",
      "Epoch 10/10\n",
      "56/56 [==============================] - 499s 9s/step - loss: 13.0420 - acc: 0.1908 - val_loss: 12.9795 - val_acc: 0.1947\n",
      "Found 210 images belonging to 6 classes.\n",
      "CONFUSION MATRIX:\n",
      "[[ 0  0  0  0 40  0]\n",
      " [ 0  0  0  0 40  0]\n",
      " [ 0  0  0  0 40  0]\n",
      " [ 0  0  0  0 40  0]\n",
      " [ 0  0  0  0 40  0]\n",
      " [ 0  0  0  0 10  0]]\n",
      "CLASSIFICATION REPORT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   cardboard       0.00      0.00      0.00        40\n",
      "       glass       0.00      0.00      0.00        40\n",
      "       metal       0.00      0.00      0.00        40\n",
      "       paper       0.00      0.00      0.00        40\n",
      "     plastic       0.19      1.00      0.32        40\n",
      "       trash       0.00      0.00      0.00        10\n",
      "\n",
      "   micro avg       0.19      0.19      0.19       210\n",
      "   macro avg       0.03      0.17      0.05       210\n",
      "weighted avg       0.04      0.19      0.06       210\n",
      "\n",
      "*************** Test  8  Batch size:  32  Epochs:  20 ***************\n",
      "Found 1792 images belonging to 6 classes.\n",
      "Found 525 images belonging to 6 classes.\n",
      "Epoch 1/20\n",
      "56/56 [==============================] - 514s 9s/step - loss: 12.8892 - acc: 0.1869 - val_loss: 13.0645 - val_acc: 0.1895\n",
      "Epoch 2/20\n",
      "56/56 [==============================] - 478s 9s/step - loss: 13.0420 - acc: 0.1908 - val_loss: 13.0776 - val_acc: 0.1886\n",
      "Epoch 3/20\n",
      "56/56 [==============================] - 481s 9s/step - loss: 13.0420 - acc: 0.1908 - val_loss: 13.0776 - val_acc: 0.1886\n",
      "Epoch 4/20\n",
      "56/56 [==============================] - 501s 9s/step - loss: 13.0420 - acc: 0.1908 - val_loss: 12.9141 - val_acc: 0.1988\n",
      "Epoch 5/20\n",
      "24/56 [===========>..................] - ETA: 3:53 - loss: 12.7392 - acc: 0.2096"
     ]
    }
   ],
   "source": [
    "runExperiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************** Test  8  Batch size:  32  Epochs:  20 ***************\n",
      "Found 1792 images belonging to 6 classes.\n",
      "Found 525 images belonging to 6 classes.\n",
      "Epoch 1/20\n",
      "56/56 [==============================] - 526s 9s/step - loss: 10.8637 - acc: 0.2997 - val_loss: 12.9808 - val_acc: 0.1934\n",
      "Epoch 2/20\n",
      "56/56 [==============================] - 479s 9s/step - loss: 10.3266 - acc: 0.3555 - val_loss: 12.9468 - val_acc: 0.1968\n",
      "Epoch 3/20\n",
      "56/56 [==============================] - 483s 9s/step - loss: 10.9366 - acc: 0.3198 - val_loss: 13.1103 - val_acc: 0.1866\n",
      "Epoch 4/20\n",
      "56/56 [==============================] - 481s 9s/step - loss: 10.9635 - acc: 0.3186 - val_loss: 13.0122 - val_acc: 0.1927\n",
      "Epoch 5/20\n",
      "56/56 [==============================] - 481s 9s/step - loss: 12.8621 - acc: 0.2020 - val_loss: 13.0449 - val_acc: 0.1907\n",
      "Epoch 6/20\n",
      "56/56 [==============================] - 479s 9s/step - loss: 12.7958 - acc: 0.2059 - val_loss: 13.0122 - val_acc: 0.1927\n",
      "Epoch 7/20\n",
      "56/56 [==============================] - 480s 9s/step - loss: 12.5923 - acc: 0.2188 - val_loss: 13.0122 - val_acc: 0.1927\n",
      "Epoch 8/20\n",
      "56/56 [==============================] - 478s 9s/step - loss: 11.9972 - acc: 0.2545 - val_loss: 13.0776 - val_acc: 0.1886\n",
      "Epoch 9/20\n",
      "56/56 [==============================] - 478s 9s/step - loss: 10.2627 - acc: 0.3610 - val_loss: 12.9141 - val_acc: 0.1988\n",
      "Epoch 10/20\n",
      "56/56 [==============================] - 475s 8s/step - loss: 10.2004 - acc: 0.3666 - val_loss: 13.2737 - val_acc: 0.1765\n",
      "Epoch 11/20\n",
      "56/56 [==============================] - 473s 8s/step - loss: 9.9281 - acc: 0.3834 - val_loss: 12.7506 - val_acc: 0.2089\n",
      "Epoch 12/20\n",
      "56/56 [==============================] - 476s 9s/step - loss: 10.1482 - acc: 0.3694 - val_loss: 12.9468 - val_acc: 0.1968\n",
      "Epoch 13/20\n",
      "56/56 [==============================] - 473s 8s/step - loss: 11.3847 - acc: 0.2930 - val_loss: 12.7833 - val_acc: 0.2069\n",
      "Epoch 14/20\n",
      "56/56 [==============================] - 500s 9s/step - loss: 10.2685 - acc: 0.3627 - val_loss: 13.3391 - val_acc: 0.1724\n",
      "Epoch 15/20\n",
      "56/56 [==============================] - 498s 9s/step - loss: 10.5592 - acc: 0.3449 - val_loss: 12.9468 - val_acc: 0.1968\n",
      "Epoch 16/20\n",
      "56/56 [==============================] - 499s 9s/step - loss: 10.8491 - acc: 0.3265 - val_loss: 13.0449 - val_acc: 0.1907\n",
      "Epoch 17/20\n",
      "56/56 [==============================] - 514s 9s/step - loss: 11.5029 - acc: 0.2857 - val_loss: 13.0776 - val_acc: 0.1886\n",
      "Epoch 18/20\n",
      "56/56 [==============================] - 498s 9s/step - loss: 10.0431 - acc: 0.3761 - val_loss: 12.9700 - val_acc: 0.1953\n",
      "Epoch 19/20\n",
      "56/56 [==============================] - 513s 9s/step - loss: 11.9273 - acc: 0.2595 - val_loss: 12.8487 - val_acc: 0.2028\n",
      "Epoch 20/20\n",
      "56/56 [==============================] - 514s 9s/step - loss: 13.6626 - acc: 0.1523 - val_loss: 13.3391 - val_acc: 0.1724\n",
      "Found 210 images belonging to 6 classes.\n",
      "CONFUSION MATRIX:\n",
      "[[ 0  0 40  0  0  0]\n",
      " [ 0  0 40  0  0  0]\n",
      " [ 0  0 40  0  0  0]\n",
      " [ 0  0 40  0  0  0]\n",
      " [ 0  0 40  0  0  0]\n",
      " [ 0  0 10  0  0  0]]\n",
      "CLASSIFICATION REPORT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   cardboard       0.00      0.00      0.00        40\n",
      "       glass       0.00      0.00      0.00        40\n",
      "       metal       0.19      1.00      0.32        40\n",
      "       paper       0.00      0.00      0.00        40\n",
      "     plastic       0.00      0.00      0.00        40\n",
      "       trash       0.00      0.00      0.00        10\n",
      "\n",
      "   micro avg       0.19      0.19      0.19       210\n",
      "   macro avg       0.03      0.17      0.05       210\n",
      "weighted avg       0.04      0.19      0.06       210\n",
      "\n",
      "*************** Test  9  Batch size:  32  Epochs:  30 ***************\n",
      "Found 1792 images belonging to 6 classes.\n",
      "Found 525 images belonging to 6 classes.\n",
      "Epoch 1/30\n",
      "56/56 [==============================] - 531s 9s/step - loss: 11.9287 - acc: 0.2478 - val_loss: 13.0645 - val_acc: 0.1895\n",
      "Epoch 2/30\n",
      "56/56 [==============================] - 497s 9s/step - loss: 12.0346 - acc: 0.2533 - val_loss: 12.9795 - val_acc: 0.1947\n",
      "Epoch 3/30\n",
      "56/56 [==============================] - 478s 9s/step - loss: 12.0346 - acc: 0.2533 - val_loss: 13.3064 - val_acc: 0.1744\n",
      "Epoch 4/30\n",
      "56/56 [==============================] - 472s 8s/step - loss: 12.0346 - acc: 0.2533 - val_loss: 12.7506 - val_acc: 0.2089\n",
      "Epoch 5/30\n",
      "56/56 [==============================] - 468s 8s/step - loss: 12.0346 - acc: 0.2533 - val_loss: 13.1756 - val_acc: 0.1826\n",
      "Epoch 6/30\n",
      "56/56 [==============================] - 470s 8s/step - loss: 12.0346 - acc: 0.2533 - val_loss: 12.8160 - val_acc: 0.2049\n",
      "Epoch 7/30\n",
      "56/56 [==============================] - 474s 8s/step - loss: 12.0346 - acc: 0.2533 - val_loss: 13.4372 - val_acc: 0.1663\n",
      "Epoch 8/30\n",
      "56/56 [==============================] - 477s 9s/step - loss: 12.0346 - acc: 0.2533 - val_loss: 12.8814 - val_acc: 0.2008\n",
      "Epoch 9/30\n",
      "56/56 [==============================] - 468s 8s/step - loss: 12.0346 - acc: 0.2533 - val_loss: 13.3064 - val_acc: 0.1744\n",
      "Epoch 10/30\n",
      "56/56 [==============================] - 469s 8s/step - loss: 12.0346 - acc: 0.2533 - val_loss: 12.7179 - val_acc: 0.2110\n",
      "Epoch 11/30\n",
      "56/56 [==============================] - 468s 8s/step - loss: 12.0346 - acc: 0.2533 - val_loss: 13.1430 - val_acc: 0.1846\n",
      "Epoch 12/30\n",
      "56/56 [==============================] - 468s 8s/step - loss: 12.0346 - acc: 0.2533 - val_loss: 12.7506 - val_acc: 0.2089\n",
      "Epoch 13/30\n",
      "56/56 [==============================] - 464s 8s/step - loss: 12.0346 - acc: 0.2533 - val_loss: 13.5026 - val_acc: 0.1623\n",
      "Epoch 14/30\n",
      "56/56 [==============================] - 470s 8s/step - loss: 12.0346 - acc: 0.2533 - val_loss: 12.8487 - val_acc: 0.2028\n",
      "Epoch 15/30\n",
      "56/56 [==============================] - 469s 8s/step - loss: 12.0346 - acc: 0.2533 - val_loss: 12.8814 - val_acc: 0.2008\n",
      "Epoch 16/30\n",
      "56/56 [==============================] - 470s 8s/step - loss: 12.0346 - acc: 0.2533 - val_loss: 13.1756 - val_acc: 0.1826\n",
      "Epoch 17/30\n",
      "56/56 [==============================] - 469s 8s/step - loss: 12.0346 - acc: 0.2533 - val_loss: 13.0776 - val_acc: 0.1886\n",
      "Epoch 18/30\n",
      "56/56 [==============================] - 474s 8s/step - loss: 12.0346 - acc: 0.2533 - val_loss: 13.0645 - val_acc: 0.1895\n",
      "Epoch 19/30\n",
      "56/56 [==============================] - 476s 9s/step - loss: 12.0346 - acc: 0.2533 - val_loss: 12.9795 - val_acc: 0.1947\n",
      "Epoch 20/30\n",
      "56/56 [==============================] - 473s 8s/step - loss: 12.0346 - acc: 0.2533 - val_loss: 13.1430 - val_acc: 0.1846\n",
      "Epoch 21/30\n",
      "56/56 [==============================] - 477s 9s/step - loss: 12.0346 - acc: 0.2533 - val_loss: 12.8814 - val_acc: 0.2008\n",
      "Epoch 22/30\n",
      "56/56 [==============================] - 470s 8s/step - loss: 12.0346 - acc: 0.2533 - val_loss: 13.2410 - val_acc: 0.1785\n",
      "Epoch 23/30\n",
      "56/56 [==============================] - 473s 8s/step - loss: 12.0346 - acc: 0.2533 - val_loss: 12.9468 - val_acc: 0.1968\n",
      "Epoch 24/30\n",
      "56/56 [==============================] - 467s 8s/step - loss: 12.0346 - acc: 0.2533 - val_loss: 13.0776 - val_acc: 0.1886\n",
      "Epoch 25/30\n",
      "56/56 [==============================] - 469s 8s/step - loss: 12.0346 - acc: 0.2533 - val_loss: 13.0122 - val_acc: 0.1927\n",
      "Epoch 26/30\n",
      "56/56 [==============================] - 467s 8s/step - loss: 12.0346 - acc: 0.2533 - val_loss: 12.8160 - val_acc: 0.2049\n",
      "Epoch 27/30\n",
      "56/56 [==============================] - 466s 8s/step - loss: 12.0346 - acc: 0.2533 - val_loss: 13.5026 - val_acc: 0.1623\n",
      "Epoch 28/30\n",
      "56/56 [==============================] - 471s 8s/step - loss: 12.0346 - acc: 0.2533 - val_loss: 12.6852 - val_acc: 0.2130\n",
      "Epoch 29/30\n",
      "56/56 [==============================] - 470s 8s/step - loss: 12.0346 - acc: 0.2533 - val_loss: 13.1430 - val_acc: 0.1846\n",
      "Epoch 30/30\n",
      "56/56 [==============================] - 461s 8s/step - loss: 12.0346 - acc: 0.2533 - val_loss: 13.2083 - val_acc: 0.1805\n",
      "Found 210 images belonging to 6 classes.\n",
      "CONFUSION MATRIX:\n",
      "[[ 0  0  0 40  0  0]\n",
      " [ 0  0  0 40  0  0]\n",
      " [ 0  0  0 40  0  0]\n",
      " [ 0  0  0 40  0  0]\n",
      " [ 0  0  0 40  0  0]\n",
      " [ 0  0  0 10  0  0]]\n",
      "CLASSIFICATION REPORT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   cardboard       0.00      0.00      0.00        40\n",
      "       glass       0.00      0.00      0.00        40\n",
      "       metal       0.00      0.00      0.00        40\n",
      "       paper       0.19      1.00      0.32        40\n",
      "     plastic       0.00      0.00      0.00        40\n",
      "       trash       0.00      0.00      0.00        10\n",
      "\n",
      "   micro avg       0.19      0.19      0.19       210\n",
      "   macro avg       0.03      0.17      0.05       210\n",
      "weighted avg       0.04      0.19      0.06       210\n",
      "\n"
     ]
    }
   ],
   "source": [
    "runExperiments()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "martin",
   "language": "python",
   "name": "martin"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
