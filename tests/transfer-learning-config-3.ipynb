{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from imgaug import augmenters as iaa\n",
    "from keras import applications, optimizers\n",
    "from keras.layers import Dropout, Flatten, Dense, Input\n",
    "from keras.models import Sequential, Model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from PIL import Image\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import cv2\n",
    "import keras\n",
    "import os, csv\n",
    "import shutil\n",
    "import imgaug as ia\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensions the images.\n",
    "img_width, img_height = 512, 384\n",
    "\n",
    "train_data_dir = '../dataset/train'\n",
    "test_data_dir = '../dataset/test'\n",
    "validation_data_dir = '../dataset/validation'\n",
    "original_data_dir = '../dataset-resized'\n",
    "\n",
    "batch_size = 16\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildModel():\n",
    "    # build the VGG16 network\n",
    "    base_model = applications.VGG16(weights='imagenet', \n",
    "                                    include_top=False,\n",
    "                                    input_tensor=Input(shape=(img_width, img_height, 3)))\n",
    "\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "        \n",
    "    top_model = base_model.output\n",
    "    top_model = Flatten(name=\"Flatten\")(top_model)\n",
    "    top_model = Dense(512, activation='relu')(top_model)\n",
    "    top_model = Dense(256, activation='relu')(top_model)\n",
    "    top_model = Dense(6, activation='softmax')(top_model)\n",
    "    \n",
    "    model = Model(inputs=base_model.input, outputs=top_model)\n",
    "\n",
    "#     model.summary()\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateData(batch_size):\n",
    "    train_datagen = ImageDataGenerator()\n",
    "\n",
    "    validation_datagen = ImageDataGenerator()\n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        class_mode='categorical')\n",
    "\n",
    "    validation_generator = validation_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')\n",
    "    \n",
    "    return [train_generator,validation_generator]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fineTuneModel(model, train_generator, validation_generator, epochs, batch_size):\n",
    "    training_samples = 0\n",
    "    validation_samples = 0\n",
    "    \n",
    "    for path, dirs, files in os.walk(train_data_dir):\n",
    "        for filename in files:\n",
    "            training_samples += 1 \n",
    "\n",
    "    for path, dirs, files in os.walk(validation_data_dir):\n",
    "        for filename in files:\n",
    "            validation_samples += 1 \n",
    "        \n",
    "    model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=training_samples // batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=validation_samples // batch_size)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMetrics(model, batch_size):\n",
    "    \n",
    "    test_samples = 0\n",
    "    \n",
    "    for path, dirs, files in os.walk(test_data_dir):\n",
    "        for filename in files:\n",
    "            test_samples += 1 \n",
    "    \n",
    "    test_datagen = ImageDataGenerator(rescale=1. /255)\n",
    "\n",
    "    test_generator = test_datagen.flow_from_directory(\n",
    "        test_data_dir,\n",
    "        target_size=(img_width,img_height),\n",
    "        shuffle=False,\n",
    "        batch_size=1,\n",
    "        class_mode='categorical')\n",
    "    \n",
    "    predicted_results = model.predict_generator(test_generator, \n",
    "                                                steps = test_samples)\n",
    "    predicted_results = np.argmax(predicted_results, axis=1)\n",
    "    targets = ['cardboard', 'glass', 'metal', 'paper', 'plastic', 'trash']\n",
    "    \n",
    "    # confusion matrix\n",
    "    print('CONFUSION MATRIX:')\n",
    "    print(confusion_matrix(test_generator.classes, predicted_results))\n",
    "    \n",
    "    # classification report\n",
    "    print('CLASSIFICATION REPORT:')\n",
    "    print(classification_report(test_generator.classes, predicted_results, target_names=targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runExperiments():\n",
    "    batch_sizes = [8,16,32]\n",
    "    epochs = [10,20,30]\n",
    "    config = 1\n",
    "    \n",
    "    for batch_size in batch_sizes:\n",
    "        for epoch in epochs:\n",
    "            \n",
    "            print(\"*************** Test \", config, \" Batch size: \", batch_size, \" Epochs: \", epoch, \"***************\")           \n",
    "            keras.backend.clear_session()\n",
    "            \n",
    "            model = buildModel()\n",
    "            train_generator,test_generator = generateData(batch_size)\n",
    "            trained_model = fineTuneModel(model,train_generator, test_generator, epoch, batch_size)\n",
    "            metrics = getMetrics(trained_model, batch_size)\n",
    "            \n",
    "            config += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************** Test  1  Batch size:  8  Epochs:  10 ***************\n",
      "Found 3572 images belonging to 6 classes.\n",
      "Found 224 images belonging to 6 classes.\n",
      "Epoch 1/10\n",
      "446/446 [==============================] - 125s 281ms/step - loss: 12.6348 - acc: 0.2152 - val_loss: 13.4557 - val_acc: 0.1652\n",
      "Epoch 2/10\n",
      "446/446 [==============================] - 124s 278ms/step - loss: 12.6307 - acc: 0.2164 - val_loss: 13.4557 - val_acc: 0.1652\n",
      "Epoch 3/10\n",
      "446/446 [==============================] - 125s 279ms/step - loss: 12.6261 - acc: 0.2166 - val_loss: 13.4557 - val_acc: 0.1652\n",
      "Epoch 4/10\n",
      "446/446 [==============================] - 125s 279ms/step - loss: 12.6487 - acc: 0.2152 - val_loss: 13.4557 - val_acc: 0.1652\n",
      "Epoch 5/10\n",
      "446/446 [==============================] - 125s 280ms/step - loss: 12.6397 - acc: 0.2158 - val_loss: 13.4557 - val_acc: 0.1652\n",
      "Epoch 6/10\n",
      "446/446 [==============================] - 125s 280ms/step - loss: 12.6216 - acc: 0.2169 - val_loss: 13.4557 - val_acc: 0.1652\n",
      "Epoch 7/10\n",
      "446/446 [==============================] - 125s 280ms/step - loss: 12.6532 - acc: 0.2150 - val_loss: 13.4557 - val_acc: 0.1652\n",
      "Epoch 8/10\n",
      "446/446 [==============================] - 125s 280ms/step - loss: 12.6442 - acc: 0.2155 - val_loss: 13.4557 - val_acc: 0.1652\n",
      "Epoch 9/10\n",
      "446/446 [==============================] - 125s 280ms/step - loss: 12.6442 - acc: 0.2155 - val_loss: 13.4557 - val_acc: 0.1652\n",
      "Epoch 10/10\n",
      "446/446 [==============================] - 125s 280ms/step - loss: 12.6307 - acc: 0.2164 - val_loss: 13.4557 - val_acc: 0.1652\n",
      "Found 447 images belonging to 6 classes.\n",
      "CONFUSION MATRIX:\n",
      "[[  0   0   0   0  81   0]\n",
      " [  0   0   0   0 100   0]\n",
      " [  0   0   0   0  82   0]\n",
      " [  0   0   0   0  60   0]\n",
      " [  0   0   0   0  97   0]\n",
      " [  0   0   0   0  27   0]]\n",
      "CLASSIFICATION REPORT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   cardboard       0.00      0.00      0.00        81\n",
      "       glass       0.00      0.00      0.00       100\n",
      "       metal       0.00      0.00      0.00        82\n",
      "       paper       0.00      0.00      0.00        60\n",
      "     plastic       0.22      1.00      0.36        97\n",
      "       trash       0.00      0.00      0.00        27\n",
      "\n",
      "   micro avg       0.22      0.22      0.22       447\n",
      "   macro avg       0.04      0.17      0.06       447\n",
      "weighted avg       0.05      0.22      0.08       447\n",
      "\n",
      "*************** Test  2  Batch size:  8  Epochs:  20 ***************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martin/anaconda3/envs/martin/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3572 images belonging to 6 classes.\n",
      "Found 224 images belonging to 6 classes.\n",
      "Epoch 1/20\n",
      "446/446 [==============================] - 127s 284ms/step - loss: 12.4924 - acc: 0.2239 - val_loss: 12.1605 - val_acc: 0.2455\n",
      "Epoch 2/20\n",
      "446/446 [==============================] - 125s 280ms/step - loss: 12.4906 - acc: 0.2251 - val_loss: 12.1605 - val_acc: 0.2455\n",
      "Epoch 3/20\n",
      "446/446 [==============================] - 125s 280ms/step - loss: 12.4951 - acc: 0.2248 - val_loss: 12.1605 - val_acc: 0.2455\n",
      "Epoch 4/20\n",
      "446/446 [==============================] - 125s 280ms/step - loss: 12.5177 - acc: 0.2234 - val_loss: 12.1605 - val_acc: 0.2455\n",
      "Epoch 5/20\n",
      "446/446 [==============================] - 125s 279ms/step - loss: 12.4816 - acc: 0.2256 - val_loss: 12.1605 - val_acc: 0.2455\n",
      "Epoch 6/20\n",
      "446/446 [==============================] - 125s 280ms/step - loss: 12.5268 - acc: 0.2228 - val_loss: 12.1605 - val_acc: 0.2455\n",
      "Epoch 7/20\n",
      "446/446 [==============================] - 125s 280ms/step - loss: 12.4951 - acc: 0.2248 - val_loss: 12.1605 - val_acc: 0.2455\n",
      "Epoch 8/20\n",
      "446/446 [==============================] - 125s 280ms/step - loss: 12.4906 - acc: 0.2251 - val_loss: 12.1605 - val_acc: 0.2455\n",
      "Epoch 9/20\n",
      "446/446 [==============================] - 125s 280ms/step - loss: 12.4997 - acc: 0.2245 - val_loss: 12.1605 - val_acc: 0.2455\n",
      "Epoch 10/20\n",
      "446/446 [==============================] - 125s 280ms/step - loss: 12.5042 - acc: 0.2242 - val_loss: 12.1605 - val_acc: 0.2455\n",
      "Epoch 11/20\n",
      "446/446 [==============================] - 125s 280ms/step - loss: 12.4861 - acc: 0.2253 - val_loss: 12.1605 - val_acc: 0.2455\n",
      "Epoch 12/20\n",
      "446/446 [==============================] - 125s 280ms/step - loss: 12.4861 - acc: 0.2253 - val_loss: 12.1605 - val_acc: 0.2455\n",
      "Epoch 13/20\n",
      "446/446 [==============================] - 125s 280ms/step - loss: 12.5177 - acc: 0.2234 - val_loss: 12.1605 - val_acc: 0.2455\n",
      "Epoch 14/20\n",
      "446/446 [==============================] - 125s 280ms/step - loss: 12.5358 - acc: 0.2223 - val_loss: 12.1605 - val_acc: 0.2455\n",
      "Epoch 15/20\n",
      "446/446 [==============================] - 125s 280ms/step - loss: 12.4861 - acc: 0.2253 - val_loss: 12.1605 - val_acc: 0.2455\n",
      "Epoch 16/20\n",
      "446/446 [==============================] - 125s 280ms/step - loss: 12.4726 - acc: 0.2262 - val_loss: 12.1605 - val_acc: 0.2455\n",
      "Epoch 17/20\n",
      "446/446 [==============================] - 125s 280ms/step - loss: 12.4680 - acc: 0.2265 - val_loss: 12.1605 - val_acc: 0.2455\n",
      "Epoch 18/20\n",
      "446/446 [==============================] - 125s 280ms/step - loss: 12.5584 - acc: 0.2209 - val_loss: 12.1605 - val_acc: 0.2455\n",
      "Epoch 19/20\n",
      "446/446 [==============================] - 125s 280ms/step - loss: 12.4184 - acc: 0.2295 - val_loss: 12.1605 - val_acc: 0.2455\n",
      "Epoch 20/20\n",
      "446/446 [==============================] - 125s 280ms/step - loss: 12.5539 - acc: 0.2211 - val_loss: 12.1605 - val_acc: 0.2455\n",
      "Found 447 images belonging to 6 classes.\n",
      "CONFUSION MATRIX:\n",
      "[[  0  81   0   0   0   0]\n",
      " [  0 100   0   0   0   0]\n",
      " [  0  82   0   0   0   0]\n",
      " [  0  60   0   0   0   0]\n",
      " [  0  97   0   0   0   0]\n",
      " [  0  27   0   0   0   0]]\n",
      "CLASSIFICATION REPORT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   cardboard       0.00      0.00      0.00        81\n",
      "       glass       0.22      1.00      0.37       100\n",
      "       metal       0.00      0.00      0.00        82\n",
      "       paper       0.00      0.00      0.00        60\n",
      "     plastic       0.00      0.00      0.00        97\n",
      "       trash       0.00      0.00      0.00        27\n",
      "\n",
      "   micro avg       0.22      0.22      0.22       447\n",
      "   macro avg       0.04      0.17      0.06       447\n",
      "weighted avg       0.05      0.22      0.08       447\n",
      "\n",
      "*************** Test  3  Batch size:  8  Epochs:  30 ***************\n",
      "Found 3572 images belonging to 6 classes.\n",
      "Found 224 images belonging to 6 classes.\n",
      "Epoch 1/30\n",
      "446/446 [==============================] - 127s 284ms/step - loss: 13.2149 - acc: 0.1794 - val_loss: 12.8081 - val_acc: 0.2054\n",
      "Epoch 2/30\n",
      "446/446 [==============================] - 125s 280ms/step - loss: 13.2179 - acc: 0.1799 - val_loss: 12.8081 - val_acc: 0.2054\n",
      "Epoch 3/30\n",
      "446/446 [==============================] - 125s 280ms/step - loss: 13.2134 - acc: 0.1802 - val_loss: 12.8081 - val_acc: 0.2054\n",
      "Epoch 4/30\n",
      "446/446 [==============================] - 125s 280ms/step - loss: 13.2089 - acc: 0.1805 - val_loss: 12.8081 - val_acc: 0.2054\n",
      "Epoch 5/30\n",
      "446/446 [==============================] - 125s 280ms/step - loss: 13.2134 - acc: 0.1802 - val_loss: 12.8081 - val_acc: 0.2054\n",
      "Epoch 6/30\n",
      "446/446 [==============================] - 125s 281ms/step - loss: 13.2270 - acc: 0.1794 - val_loss: 12.8081 - val_acc: 0.2054\n",
      "Epoch 7/30\n",
      "446/446 [==============================] - 125s 281ms/step - loss: 13.1953 - acc: 0.1813 - val_loss: 12.8081 - val_acc: 0.2054\n",
      "Epoch 8/30\n",
      "446/446 [==============================] - 125s 280ms/step - loss: 13.1999 - acc: 0.1811 - val_loss: 12.8081 - val_acc: 0.2054\n",
      "Epoch 9/30\n",
      "446/446 [==============================] - 125s 280ms/step - loss: 13.2270 - acc: 0.1794 - val_loss: 12.8081 - val_acc: 0.2054\n",
      "Epoch 10/30\n",
      "446/446 [==============================] - 125s 280ms/step - loss: 13.2179 - acc: 0.1799 - val_loss: 12.8081 - val_acc: 0.2054\n",
      "Epoch 11/30\n",
      "446/446 [==============================] - 125s 280ms/step - loss: 13.2360 - acc: 0.1788 - val_loss: 12.8081 - val_acc: 0.2054\n",
      "Epoch 12/30\n",
      "446/446 [==============================] - 125s 281ms/step - loss: 13.1773 - acc: 0.1825 - val_loss: 12.8081 - val_acc: 0.2054\n",
      "Epoch 13/30\n",
      "446/446 [==============================] - 125s 281ms/step - loss: 13.1773 - acc: 0.1825 - val_loss: 12.8081 - val_acc: 0.2054\n",
      "Epoch 14/30\n",
      "446/446 [==============================] - 125s 280ms/step - loss: 13.2766 - acc: 0.1763 - val_loss: 12.8081 - val_acc: 0.2054\n",
      "Epoch 15/30\n",
      "446/446 [==============================] - 125s 280ms/step - loss: 13.1953 - acc: 0.1813 - val_loss: 12.8081 - val_acc: 0.2054\n",
      "Epoch 16/30\n",
      "446/446 [==============================] - 125s 280ms/step - loss: 13.1863 - acc: 0.1819 - val_loss: 12.8081 - val_acc: 0.2054\n",
      "Epoch 17/30\n",
      "446/446 [==============================] - 125s 281ms/step - loss: 13.2450 - acc: 0.1783 - val_loss: 12.8081 - val_acc: 0.2054\n",
      "Epoch 18/30\n",
      "446/446 [==============================] - 125s 281ms/step - loss: 13.2179 - acc: 0.1799 - val_loss: 12.8081 - val_acc: 0.2054\n",
      "Epoch 19/30\n",
      "446/446 [==============================] - 125s 280ms/step - loss: 13.2134 - acc: 0.1802 - val_loss: 12.8081 - val_acc: 0.2054\n",
      "Epoch 20/30\n",
      "446/446 [==============================] - 125s 281ms/step - loss: 13.1682 - acc: 0.1830 - val_loss: 12.8081 - val_acc: 0.2054\n",
      "Epoch 21/30\n",
      "446/446 [==============================] - 125s 281ms/step - loss: 13.2360 - acc: 0.1788 - val_loss: 12.8081 - val_acc: 0.2054\n",
      "Epoch 22/30\n",
      "446/446 [==============================] - 125s 281ms/step - loss: 13.2315 - acc: 0.1791 - val_loss: 12.8081 - val_acc: 0.2054\n",
      "Epoch 23/30\n",
      "446/446 [==============================] - 125s 281ms/step - loss: 13.1953 - acc: 0.1813 - val_loss: 12.8081 - val_acc: 0.2054\n",
      "Epoch 24/30\n",
      "446/446 [==============================] - 125s 281ms/step - loss: 13.2315 - acc: 0.1791 - val_loss: 12.8081 - val_acc: 0.2054\n",
      "Epoch 25/30\n",
      "446/446 [==============================] - 125s 281ms/step - loss: 13.1592 - acc: 0.1836 - val_loss: 12.8081 - val_acc: 0.2054\n",
      "Epoch 26/30\n",
      "446/446 [==============================] - 125s 281ms/step - loss: 13.2224 - acc: 0.1797 - val_loss: 12.8081 - val_acc: 0.2054\n",
      "Epoch 27/30\n",
      "446/446 [==============================] - 125s 280ms/step - loss: 13.2767 - acc: 0.1763 - val_loss: 12.8081 - val_acc: 0.2054\n",
      "Epoch 28/30\n",
      "446/446 [==============================] - 125s 280ms/step - loss: 13.1998 - acc: 0.1811 - val_loss: 12.8081 - val_acc: 0.2054\n",
      "Epoch 29/30\n",
      "446/446 [==============================] - 125s 280ms/step - loss: 13.1863 - acc: 0.1819 - val_loss: 12.8081 - val_acc: 0.2054\n",
      "Epoch 30/30\n",
      "446/446 [==============================] - 125s 280ms/step - loss: 13.1547 - acc: 0.1839 - val_loss: 12.8081 - val_acc: 0.2054\n",
      "Found 447 images belonging to 6 classes.\n",
      "CONFUSION MATRIX:\n",
      "[[ 81   0   0   0   0   0]\n",
      " [100   0   0   0   0   0]\n",
      " [ 82   0   0   0   0   0]\n",
      " [ 60   0   0   0   0   0]\n",
      " [ 97   0   0   0   0   0]\n",
      " [ 27   0   0   0   0   0]]\n",
      "CLASSIFICATION REPORT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   cardboard       0.18      1.00      0.31        81\n",
      "       glass       0.00      0.00      0.00       100\n",
      "       metal       0.00      0.00      0.00        82\n",
      "       paper       0.00      0.00      0.00        60\n",
      "     plastic       0.00      0.00      0.00        97\n",
      "       trash       0.00      0.00      0.00        27\n",
      "\n",
      "   micro avg       0.18      0.18      0.18       447\n",
      "   macro avg       0.03      0.17      0.05       447\n",
      "weighted avg       0.03      0.18      0.06       447\n",
      "\n",
      "*************** Test  4  Batch size:  16  Epochs:  10 ***************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3572 images belonging to 6 classes.\n",
      "Found 224 images belonging to 6 classes.\n",
      "Epoch 1/10\n",
      "223/223 [==============================] - 110s 495ms/step - loss: 13.8684 - acc: 0.1376 - val_loss: 14.2472 - val_acc: 0.1161\n",
      "Epoch 2/10\n",
      "223/223 [==============================] - 107s 482ms/step - loss: 13.9407 - acc: 0.1351 - val_loss: 14.2472 - val_acc: 0.1161\n",
      "Epoch 3/10\n",
      "223/223 [==============================] - 107s 478ms/step - loss: 13.9453 - acc: 0.1348 - val_loss: 14.2472 - val_acc: 0.1161\n",
      "Epoch 4/10\n",
      "223/223 [==============================] - 109s 488ms/step - loss: 13.9678 - acc: 0.1334 - val_loss: 14.2472 - val_acc: 0.1161\n",
      "Epoch 5/10\n",
      "223/223 [==============================] - 108s 485ms/step - loss: 13.9272 - acc: 0.1359 - val_loss: 14.2472 - val_acc: 0.1161\n",
      "Epoch 6/10\n",
      "223/223 [==============================] - 108s 486ms/step - loss: 13.9723 - acc: 0.1331 - val_loss: 14.2472 - val_acc: 0.1161\n",
      "Epoch 7/10\n",
      "223/223 [==============================] - 106s 477ms/step - loss: 13.9272 - acc: 0.1359 - val_loss: 14.2472 - val_acc: 0.1161\n",
      "Epoch 8/10\n",
      "223/223 [==============================] - 107s 482ms/step - loss: 13.9678 - acc: 0.1334 - val_loss: 14.2472 - val_acc: 0.1161\n",
      "Epoch 9/10\n",
      "223/223 [==============================] - 106s 478ms/step - loss: 13.9813 - acc: 0.1326 - val_loss: 14.2472 - val_acc: 0.1161\n",
      "Epoch 10/10\n",
      "223/223 [==============================] - 107s 480ms/step - loss: 13.9407 - acc: 0.1351 - val_loss: 14.2472 - val_acc: 0.1161\n",
      "Found 447 images belonging to 6 classes.\n",
      "CONFUSION MATRIX:\n",
      "[[  0   0   0  81   0   0]\n",
      " [  0   0   0 100   0   0]\n",
      " [  0   0   0  82   0   0]\n",
      " [  0   0   0  60   0   0]\n",
      " [  0   0   0  97   0   0]\n",
      " [  0   0   0  27   0   0]]\n",
      "CLASSIFICATION REPORT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   cardboard       0.00      0.00      0.00        81\n",
      "       glass       0.00      0.00      0.00       100\n",
      "       metal       0.00      0.00      0.00        82\n",
      "       paper       0.13      1.00      0.24        60\n",
      "     plastic       0.00      0.00      0.00        97\n",
      "       trash       0.00      0.00      0.00        27\n",
      "\n",
      "   micro avg       0.13      0.13      0.13       447\n",
      "   macro avg       0.02      0.17      0.04       447\n",
      "weighted avg       0.02      0.13      0.03       447\n",
      "\n",
      "*************** Test  5  Batch size:  16  Epochs:  20 ***************\n",
      "Found 3572 images belonging to 6 classes.\n",
      "Found 224 images belonging to 6 classes.\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "2 root error(s) found.\n  (0) Resource exhausted: OOM when allocating tensor with shape[16,64,512,384] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node block1_conv2/convolution}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[loss/mul/_247]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n  (1) Resource exhausted: OOM when allocating tensor with shape[16,64,512,384] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node block1_conv2/convolution}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n0 successful operations.\n0 derived errors ignored.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-35ff16d54f24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrunExperiments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-4a478d34b6e2>\u001b[0m in \u001b[0;36mrunExperiments\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuildModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerateData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mtrained_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfineTuneModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetMetrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrained_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-0d0cc24e7ef7>\u001b[0m in \u001b[0;36mfineTuneModel\u001b[0;34m(model, train_generator, validation_generator, epochs, batch_size)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         validation_steps=validation_samples // batch_size)\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/martin/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/martin/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/martin/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/martin/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/martin/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/martin/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/martin/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: 2 root error(s) found.\n  (0) Resource exhausted: OOM when allocating tensor with shape[16,64,512,384] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node block1_conv2/convolution}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[loss/mul/_247]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n  (1) Resource exhausted: OOM when allocating tensor with shape[16,64,512,384] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node block1_conv2/convolution}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n0 successful operations.\n0 derived errors ignored."
     ]
    }
   ],
   "source": [
    "runExperiments()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "martin",
   "language": "python",
   "name": "martin"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
