{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import applications, optimizers\n",
    "from keras.layers import Dropout, Flatten, Dense, Input\n",
    "from keras.models import Sequential, Model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensions of our images.\n",
    "img_width, img_height = 224, 224\n",
    "\n",
    "train_data_dir = '../dataset/train'\n",
    "test_data_dir = '../dataset/test'\n",
    "validation_data_dir = '../dataset/validation'\n",
    "nb_train_samples = 1792\n",
    "nb_test_samples = 210\n",
    "nb_validation_samples = 525"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildModel():\n",
    "    # build the VGG16 network\n",
    "    base_model = applications.VGG16(weights='imagenet', \n",
    "                                    include_top=False,\n",
    "                                    input_tensor=Input(shape=(img_width, img_height, 3)))\n",
    "\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "        \n",
    "    top_model = base_model.output\n",
    "    top_model = Flatten(name=\"Flatten\")(top_model)\n",
    "    top_model = Dense(512, activation='relu')(top_model)\n",
    "    top_model = Dense(256, activation='relu')(top_model)\n",
    "    top_model = Dense(6, activation='softmax')(top_model)\n",
    "    \n",
    "    model = Model(inputs=base_model.input, outputs=top_model)\n",
    "\n",
    "    #     model.summary()\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateData(batch_size):\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        horizontal_flip=True,\n",
    "        rescale=1. /255,\n",
    "        rotation_range=30,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2)\n",
    "\n",
    "    validation_datagen = ImageDataGenerator(rescale=1. /255)\n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        class_mode='categorical')\n",
    "\n",
    "    validation_generator = validation_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')\n",
    "    \n",
    "    return [train_generator,validation_generator]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fineTuneModel(model, train_generator, validation_generator, epochs, batch_size):\n",
    "    model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=nb_train_samples // batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=nb_validation_samples // batch_size)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMetrics(model, batch_size):\n",
    "    \n",
    "    test_datagen = ImageDataGenerator(rescale=1. /255)\n",
    "\n",
    "    test_generator = test_datagen.flow_from_directory(\n",
    "        test_data_dir,\n",
    "        target_size=(img_width,img_height),\n",
    "        shuffle=False,\n",
    "        batch_size=1,\n",
    "        class_mode='categorical')\n",
    "    \n",
    "    predicted_results = model.predict_generator(test_generator, \n",
    "                                                steps = nb_test_samples)\n",
    "    predicted_results = np.argmax(predicted_results, axis=1)\n",
    "    targets = [\"cardboard\", \"glass\", \"metal\", \"paper\", \"plastic\", \"trash\"]\n",
    "    \n",
    "    # confusion matrix\n",
    "    print(\"CONFUSION MATRIX:\")\n",
    "    print(confusion_matrix(test_generator.classes, predicted_results))\n",
    "    \n",
    "    # classification report\n",
    "    print(\"CLASSIFICATION REPORT:\")\n",
    "    print(classification_report(test_generator.classes, predicted_results, target_names=targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runExperiments():\n",
    "    batch_sizes = [8,16,32]\n",
    "    epochs = [10,20,30]\n",
    "    config = 1\n",
    "    \n",
    "    for batch_size in batch_sizes:\n",
    "        for epoch in epochs:\n",
    "            \n",
    "            print(\"*************** Test \", config, \" Batch size: \", batch_size, \" Epochs: \", epoch, \"***************\")           \n",
    "            keras.backend.clear_session()\n",
    "            \n",
    "            model = buildModel()\n",
    "            train_generator,test_generator = generateData(batch_size)\n",
    "            trained_model = fineTuneModel(model,train_generator, test_generator, epoch, batch_size)\n",
    "            metrics = getMetrics(trained_model, batch_size)\n",
    "            \n",
    "            config += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0803 15:19:32.201968 139791472756480 deprecation_wrapper.py:119] From /home/martin/anaconda3/envs/martin/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:95: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "W0803 15:19:32.204038 139791472756480 deprecation_wrapper.py:119] From /home/martin/anaconda3/envs/martin/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:98: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0803 15:19:32.222422 139791472756480 deprecation_wrapper.py:119] From /home/martin/anaconda3/envs/martin/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:102: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0803 15:19:32.223049 139791472756480 deprecation_wrapper.py:119] From /home/martin/anaconda3/envs/martin/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0803 15:19:32.224833 139791472756480 deprecation_wrapper.py:119] From /home/martin/anaconda3/envs/martin/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0803 15:19:32.245734 139791472756480 deprecation_wrapper.py:119] From /home/martin/anaconda3/envs/martin/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************** Test  1  Batch size:  8  Epochs:  10 ***************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0803 15:19:33.286929 139791472756480 deprecation_wrapper.py:119] From /home/martin/anaconda3/envs/martin/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1792 images belonging to 6 classes.\n",
      "Found 525 images belonging to 6 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0803 15:19:33.591894 139791472756480 deprecation.py:323] From /home/martin/anaconda3/envs/martin/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "224/224 [==============================] - 593s 3s/step - loss: 1.5086 - acc: 0.4693 - val_loss: 1.0890 - val_acc: 0.5846\n",
      "Epoch 2/10\n",
      "224/224 [==============================] - 569s 3s/step - loss: 0.9993 - acc: 0.6222 - val_loss: 0.7833 - val_acc: 0.7176\n",
      "Epoch 3/10\n",
      "224/224 [==============================] - 566s 3s/step - loss: 0.7872 - acc: 0.7081 - val_loss: 0.7042 - val_acc: 0.7331\n",
      "Epoch 4/10\n",
      "224/224 [==============================] - 571s 3s/step - loss: 0.7114 - acc: 0.7349 - val_loss: 0.6912 - val_acc: 0.7215\n",
      "Epoch 5/10\n",
      "224/224 [==============================] - 569s 3s/step - loss: 0.6962 - acc: 0.7310 - val_loss: 0.8116 - val_acc: 0.7041\n",
      "Epoch 6/10\n",
      "224/224 [==============================] - 567s 3s/step - loss: 0.6322 - acc: 0.7573 - val_loss: 0.6240 - val_acc: 0.7621\n",
      "Epoch 7/10\n",
      "224/224 [==============================] - 567s 3s/step - loss: 0.5830 - acc: 0.7801 - val_loss: 0.6393 - val_acc: 0.7660\n",
      "Epoch 8/10\n",
      "224/224 [==============================] - 562s 3s/step - loss: 0.5038 - acc: 0.8237 - val_loss: 0.8266 - val_acc: 0.7118\n",
      "Epoch 9/10\n",
      "224/224 [==============================] - 562s 3s/step - loss: 0.4494 - acc: 0.8281 - val_loss: 0.6380 - val_acc: 0.7930\n",
      "Epoch 10/10\n",
      "224/224 [==============================] - 563s 3s/step - loss: 0.4617 - acc: 0.8292 - val_loss: 0.6951 - val_acc: 0.7718\n",
      "Found 210 images belonging to 6 classes.\n",
      "CONFUSION MATRIX:\n",
      "[[32  2  0  6  0  0]\n",
      " [ 1 29  0  0 10  0]\n",
      " [ 1  8 23  1  3  4]\n",
      " [ 7  0  1 28  4  0]\n",
      " [ 0  5  0  0 35  0]\n",
      " [ 1  0  0  3  1  5]]\n",
      "CLASSIFICATION REPORT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   cardboard       0.76      0.80      0.78        40\n",
      "       glass       0.66      0.72      0.69        40\n",
      "       metal       0.96      0.57      0.72        40\n",
      "       paper       0.74      0.70      0.72        40\n",
      "     plastic       0.66      0.88      0.75        40\n",
      "       trash       0.56      0.50      0.53        10\n",
      "\n",
      "   micro avg       0.72      0.72      0.72       210\n",
      "   macro avg       0.72      0.70      0.70       210\n",
      "weighted avg       0.75      0.72      0.72       210\n",
      "\n",
      "*************** Test  2  Batch size:  8  Epochs:  20 ***************\n",
      "Found 1792 images belonging to 6 classes.\n",
      "Found 525 images belonging to 6 classes.\n",
      "Epoch 1/20\n",
      "224/224 [==============================] - 590s 3s/step - loss: 1.8734 - acc: 0.4632 - val_loss: 0.8953 - val_acc: 0.6673\n",
      "Epoch 2/20\n",
      "224/224 [==============================] - 564s 3s/step - loss: 0.9043 - acc: 0.6551 - val_loss: 0.7020 - val_acc: 0.7602\n",
      "Epoch 3/20\n",
      "224/224 [==============================] - 563s 3s/step - loss: 0.7999 - acc: 0.6881 - val_loss: 0.7276 - val_acc: 0.7215\n",
      "Epoch 4/20\n",
      "224/224 [==============================] - 567s 3s/step - loss: 0.7214 - acc: 0.7372 - val_loss: 0.7069 - val_acc: 0.7195\n",
      "Epoch 5/20\n",
      "224/224 [==============================] - 560s 3s/step - loss: 0.6339 - acc: 0.7640 - val_loss: 0.5827 - val_acc: 0.8124\n",
      "Epoch 6/20\n",
      "224/224 [==============================] - 561s 3s/step - loss: 0.5306 - acc: 0.8080 - val_loss: 0.6526 - val_acc: 0.7621\n",
      "Epoch 7/20\n",
      "224/224 [==============================] - 559s 2s/step - loss: 0.5357 - acc: 0.8002 - val_loss: 0.6468 - val_acc: 0.7872\n",
      "Epoch 8/20\n",
      "224/224 [==============================] - 559s 2s/step - loss: 0.5017 - acc: 0.8170 - val_loss: 0.9381 - val_acc: 0.7118\n",
      "Epoch 9/20\n",
      "224/224 [==============================] - 558s 2s/step - loss: 0.4109 - acc: 0.8532 - val_loss: 0.8380 - val_acc: 0.7273\n",
      "Epoch 10/20\n",
      "224/224 [==============================] - 560s 2s/step - loss: 0.4365 - acc: 0.8415 - val_loss: 0.6082 - val_acc: 0.8085\n",
      "Epoch 11/20\n",
      "224/224 [==============================] - 560s 3s/step - loss: 0.4190 - acc: 0.8471 - val_loss: 0.6630 - val_acc: 0.7872\n",
      "Epoch 12/20\n",
      "224/224 [==============================] - 561s 3s/step - loss: 0.3923 - acc: 0.8555 - val_loss: 0.6718 - val_acc: 0.8085\n",
      "Epoch 13/20\n",
      "224/224 [==============================] - 559s 2s/step - loss: 0.3561 - acc: 0.8683 - val_loss: 0.8743 - val_acc: 0.7698\n",
      "Epoch 14/20\n",
      "224/224 [==============================] - 559s 2s/step - loss: 0.3264 - acc: 0.8772 - val_loss: 0.6123 - val_acc: 0.7969\n",
      "Epoch 15/20\n",
      "224/224 [==============================] - 559s 2s/step - loss: 0.3264 - acc: 0.8912 - val_loss: 0.8997 - val_acc: 0.7505\n",
      "Epoch 16/20\n",
      "224/224 [==============================] - 561s 3s/step - loss: 0.2968 - acc: 0.8934 - val_loss: 0.7799 - val_acc: 0.7544\n",
      "Epoch 17/20\n",
      "224/224 [==============================] - 559s 2s/step - loss: 0.2798 - acc: 0.8956 - val_loss: 0.6303 - val_acc: 0.8046\n",
      "Epoch 18/20\n",
      "224/224 [==============================] - 559s 2s/step - loss: 0.2498 - acc: 0.9146 - val_loss: 0.7815 - val_acc: 0.7892\n",
      "Epoch 19/20\n",
      "224/224 [==============================] - 560s 2s/step - loss: 0.2796 - acc: 0.9007 - val_loss: 0.6595 - val_acc: 0.7892\n",
      "Epoch 20/20\n",
      "224/224 [==============================] - 559s 2s/step - loss: 0.2403 - acc: 0.9135 - val_loss: 0.8896 - val_acc: 0.7872\n",
      "Found 210 images belonging to 6 classes.\n",
      "CONFUSION MATRIX:\n",
      "[[26  0  0 14  0  0]\n",
      " [ 1 25  7  1  6  0]\n",
      " [ 0  4 32  2  0  2]\n",
      " [ 1  0  1 38  0  0]\n",
      " [ 0  1 11  4 23  1]\n",
      " [ 0  0  0  4  1  5]]\n",
      "CLASSIFICATION REPORT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   cardboard       0.93      0.65      0.76        40\n",
      "       glass       0.83      0.62      0.71        40\n",
      "       metal       0.63      0.80      0.70        40\n",
      "       paper       0.60      0.95      0.74        40\n",
      "     plastic       0.77      0.57      0.66        40\n",
      "       trash       0.62      0.50      0.56        10\n",
      "\n",
      "   micro avg       0.71      0.71      0.71       210\n",
      "   macro avg       0.73      0.68      0.69       210\n",
      "weighted avg       0.75      0.71      0.71       210\n",
      "\n",
      "*************** Test  3  Batch size:  8  Epochs:  30 ***************\n",
      "Found 1792 images belonging to 6 classes.\n",
      "Found 525 images belonging to 6 classes.\n",
      "Epoch 1/30\n",
      "224/224 [==============================] - 585s 3s/step - loss: 1.5330 - acc: 0.4788 - val_loss: 0.8786 - val_acc: 0.6538\n",
      "Epoch 2/30\n",
      "224/224 [==============================] - 564s 3s/step - loss: 0.9448 - acc: 0.6261 - val_loss: 0.7183 - val_acc: 0.7369\n",
      "Epoch 3/30\n",
      "224/224 [==============================] - 556s 2s/step - loss: 0.8074 - acc: 0.6925 - val_loss: 0.6878 - val_acc: 0.7524\n",
      "Epoch 4/30\n",
      "224/224 [==============================] - 556s 2s/step - loss: 0.6919 - acc: 0.7455 - val_loss: 0.5789 - val_acc: 0.7698\n",
      "Epoch 5/30\n",
      "224/224 [==============================] - 552s 2s/step - loss: 0.6777 - acc: 0.7372 - val_loss: 0.6374 - val_acc: 0.7872\n",
      "Epoch 6/30\n",
      "224/224 [==============================] - 552s 2s/step - loss: 0.5784 - acc: 0.7879 - val_loss: 0.6473 - val_acc: 0.7776\n",
      "Epoch 7/30\n",
      "224/224 [==============================] - 553s 2s/step - loss: 0.5484 - acc: 0.8097 - val_loss: 0.6228 - val_acc: 0.8027\n",
      "Epoch 8/30\n",
      "224/224 [==============================] - 552s 2s/step - loss: 0.4607 - acc: 0.8343 - val_loss: 0.7104 - val_acc: 0.7582\n",
      "Epoch 9/30\n",
      "224/224 [==============================] - 552s 2s/step - loss: 0.4790 - acc: 0.8326 - val_loss: 0.6314 - val_acc: 0.8104\n",
      "Epoch 10/30\n",
      "224/224 [==============================] - 553s 2s/step - loss: 0.4201 - acc: 0.8488 - val_loss: 0.6145 - val_acc: 0.7930\n",
      "Epoch 11/30\n",
      "224/224 [==============================] - 553s 2s/step - loss: 0.4324 - acc: 0.8460 - val_loss: 0.6532 - val_acc: 0.7814\n",
      "Epoch 12/30\n",
      "224/224 [==============================] - 553s 2s/step - loss: 0.4209 - acc: 0.8438 - val_loss: 0.6265 - val_acc: 0.7660\n",
      "Epoch 13/30\n",
      "224/224 [==============================] - 552s 2s/step - loss: 0.3591 - acc: 0.8683 - val_loss: 0.5883 - val_acc: 0.8027\n",
      "Epoch 14/30\n",
      "224/224 [==============================] - 554s 2s/step - loss: 0.3264 - acc: 0.8906 - val_loss: 0.5599 - val_acc: 0.8143\n",
      "Epoch 15/30\n",
      "224/224 [==============================] - 552s 2s/step - loss: 0.2841 - acc: 0.9007 - val_loss: 0.6959 - val_acc: 0.7853\n",
      "Epoch 16/30\n",
      "224/224 [==============================] - 552s 2s/step - loss: 0.3358 - acc: 0.8756 - val_loss: 0.5115 - val_acc: 0.8259\n",
      "Epoch 17/30\n",
      "224/224 [==============================] - 551s 2s/step - loss: 0.2820 - acc: 0.8917 - val_loss: 0.7630 - val_acc: 0.7756\n",
      "Epoch 18/30\n",
      "224/224 [==============================] - 553s 2s/step - loss: 0.2811 - acc: 0.8984 - val_loss: 0.7668 - val_acc: 0.7756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/30\n",
      "224/224 [==============================] - 554s 2s/step - loss: 0.2963 - acc: 0.8956 - val_loss: 0.5902 - val_acc: 0.8008\n",
      "Epoch 20/30\n",
      "224/224 [==============================] - 554s 2s/step - loss: 0.2863 - acc: 0.8945 - val_loss: 0.7000 - val_acc: 0.8066\n",
      "Epoch 21/30\n",
      "224/224 [==============================] - 553s 2s/step - loss: 0.2727 - acc: 0.9035 - val_loss: 0.6086 - val_acc: 0.7969\n",
      "Epoch 22/30\n",
      "224/224 [==============================] - 551s 2s/step - loss: 0.2440 - acc: 0.9169 - val_loss: 0.8462 - val_acc: 0.7930\n",
      "Epoch 23/30\n",
      "224/224 [==============================] - 552s 2s/step - loss: 0.2271 - acc: 0.9185 - val_loss: 0.7223 - val_acc: 0.8085\n",
      "Epoch 24/30\n",
      "224/224 [==============================] - 552s 2s/step - loss: 0.2493 - acc: 0.9090 - val_loss: 0.6340 - val_acc: 0.8143\n",
      "Epoch 25/30\n",
      "224/224 [==============================] - 552s 2s/step - loss: 0.2042 - acc: 0.9286 - val_loss: 0.7231 - val_acc: 0.7892\n",
      "Epoch 26/30\n",
      "224/224 [==============================] - 552s 2s/step - loss: 0.1992 - acc: 0.9302 - val_loss: 0.7419 - val_acc: 0.7834\n",
      "Epoch 27/30\n",
      "224/224 [==============================] - 553s 2s/step - loss: 0.1923 - acc: 0.9302 - val_loss: 0.7124 - val_acc: 0.8066\n",
      "Epoch 28/30\n",
      "224/224 [==============================] - 551s 2s/step - loss: 0.2239 - acc: 0.9174 - val_loss: 0.7140 - val_acc: 0.8124\n",
      "Epoch 29/30\n",
      "224/224 [==============================] - 557s 2s/step - loss: 0.1617 - acc: 0.9420 - val_loss: 0.8100 - val_acc: 0.7872\n",
      "Epoch 30/30\n",
      "224/224 [==============================] - 552s 2s/step - loss: 0.1853 - acc: 0.9336 - val_loss: 0.8262 - val_acc: 0.8027\n",
      "Found 210 images belonging to 6 classes.\n",
      "CONFUSION MATRIX:\n",
      "[[27  4  0  9  0  0]\n",
      " [ 0 36  2  0  2  0]\n",
      " [ 0  7 29  2  1  1]\n",
      " [ 4  2  1 32  0  1]\n",
      " [ 0 20  1  0 19  0]\n",
      " [ 1  2  0  3  1  3]]\n",
      "CLASSIFICATION REPORT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   cardboard       0.84      0.68      0.75        40\n",
      "       glass       0.51      0.90      0.65        40\n",
      "       metal       0.88      0.72      0.79        40\n",
      "       paper       0.70      0.80      0.74        40\n",
      "     plastic       0.83      0.47      0.60        40\n",
      "       trash       0.60      0.30      0.40        10\n",
      "\n",
      "   micro avg       0.70      0.70      0.70       210\n",
      "   macro avg       0.73      0.65      0.66       210\n",
      "weighted avg       0.74      0.70      0.69       210\n",
      "\n",
      "*************** Test  4  Batch size:  16  Epochs:  10 ***************\n",
      "Found 1792 images belonging to 6 classes.\n",
      "Found 525 images belonging to 6 classes.\n",
      "Epoch 1/10\n",
      "112/112 [==============================] - 556s 5s/step - loss: 4.2814 - acc: 0.4230 - val_loss: 1.2036 - val_acc: 0.5195\n",
      "Epoch 2/10\n",
      "112/112 [==============================] - 512s 5s/step - loss: 0.9789 - acc: 0.6272 - val_loss: 0.7030 - val_acc: 0.7289\n",
      "Epoch 3/10\n",
      "112/112 [==============================] - 512s 5s/step - loss: 0.8506 - acc: 0.6830 - val_loss: 0.7308 - val_acc: 0.7230\n",
      "Epoch 4/10\n",
      "112/112 [==============================] - 512s 5s/step - loss: 0.7493 - acc: 0.7277 - val_loss: 0.6559 - val_acc: 0.7662\n",
      "Epoch 5/10\n",
      "112/112 [==============================] - 508s 5s/step - loss: 0.6000 - acc: 0.7790 - val_loss: 0.6616 - val_acc: 0.7682\n",
      "Epoch 6/10\n",
      "112/112 [==============================] - 510s 5s/step - loss: 0.5789 - acc: 0.7846 - val_loss: 0.5791 - val_acc: 0.7976\n",
      "Epoch 7/10\n",
      "112/112 [==============================] - 511s 5s/step - loss: 0.4814 - acc: 0.8326 - val_loss: 0.6064 - val_acc: 0.7760\n",
      "Epoch 8/10\n",
      "112/112 [==============================] - 511s 5s/step - loss: 0.4532 - acc: 0.8315 - val_loss: 0.6124 - val_acc: 0.8094\n",
      "Epoch 9/10\n",
      "112/112 [==============================] - 511s 5s/step - loss: 0.4202 - acc: 0.8499 - val_loss: 0.7030 - val_acc: 0.7760\n",
      "Epoch 10/10\n",
      "112/112 [==============================] - 510s 5s/step - loss: 0.3767 - acc: 0.8571 - val_loss: 0.6648 - val_acc: 0.7898\n",
      "Found 210 images belonging to 6 classes.\n",
      "CONFUSION MATRIX:\n",
      "[[28  0  0 12  0  0]\n",
      " [ 1 23 11  0  5  0]\n",
      " [ 0  3 34  2  1  0]\n",
      " [ 3  0  1 35  1  0]\n",
      " [ 0  4  6  2 28  0]\n",
      " [ 0  1  0  4  1  4]]\n",
      "CLASSIFICATION REPORT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   cardboard       0.88      0.70      0.78        40\n",
      "       glass       0.74      0.57      0.65        40\n",
      "       metal       0.65      0.85      0.74        40\n",
      "       paper       0.64      0.88      0.74        40\n",
      "     plastic       0.78      0.70      0.74        40\n",
      "       trash       1.00      0.40      0.57        10\n",
      "\n",
      "   micro avg       0.72      0.72      0.72       210\n",
      "   macro avg       0.78      0.68      0.70       210\n",
      "weighted avg       0.75      0.72      0.72       210\n",
      "\n",
      "*************** Test  5  Batch size:  16  Epochs:  20 ***************\n",
      "Found 1792 images belonging to 6 classes.\n",
      "Found 525 images belonging to 6 classes.\n",
      "Epoch 1/20\n",
      "112/112 [==============================] - 556s 5s/step - loss: 2.4218 - acc: 0.4827 - val_loss: 0.8901 - val_acc: 0.6855\n",
      "Epoch 2/20\n",
      "112/112 [==============================] - 510s 5s/step - loss: 0.9429 - acc: 0.6473 - val_loss: 0.7347 - val_acc: 0.7446\n",
      "Epoch 3/20\n",
      "112/112 [==============================] - 508s 5s/step - loss: 0.7898 - acc: 0.7054 - val_loss: 0.7725 - val_acc: 0.7328\n",
      "Epoch 4/20\n",
      "112/112 [==============================] - 508s 5s/step - loss: 0.7193 - acc: 0.7249 - val_loss: 0.6580 - val_acc: 0.7682\n",
      "Epoch 5/20\n",
      "112/112 [==============================] - 510s 5s/step - loss: 0.6172 - acc: 0.7690 - val_loss: 0.5130 - val_acc: 0.8114\n",
      "Epoch 6/20\n",
      "112/112 [==============================] - 508s 5s/step - loss: 0.5358 - acc: 0.7997 - val_loss: 0.6662 - val_acc: 0.7662\n",
      "Epoch 7/20\n",
      "112/112 [==============================] - 510s 5s/step - loss: 0.5256 - acc: 0.8142 - val_loss: 0.5913 - val_acc: 0.7780\n",
      "Epoch 8/20\n",
      "112/112 [==============================] - 510s 5s/step - loss: 0.4665 - acc: 0.8376 - val_loss: 0.6631 - val_acc: 0.7741\n",
      "Epoch 9/20\n",
      "112/112 [==============================] - 509s 5s/step - loss: 0.4248 - acc: 0.8432 - val_loss: 0.6232 - val_acc: 0.7976\n",
      "Epoch 10/20\n",
      "112/112 [==============================] - 512s 5s/step - loss: 0.4233 - acc: 0.8421 - val_loss: 0.8227 - val_acc: 0.7485\n",
      "Epoch 11/20\n",
      "112/112 [==============================] - 510s 5s/step - loss: 0.4281 - acc: 0.8449 - val_loss: 0.6438 - val_acc: 0.8075\n",
      "Epoch 12/20\n",
      "112/112 [==============================] - 512s 5s/step - loss: 0.3961 - acc: 0.8627 - val_loss: 0.7935 - val_acc: 0.7367\n",
      "Epoch 13/20\n",
      "112/112 [==============================] - 510s 5s/step - loss: 0.3584 - acc: 0.8661 - val_loss: 0.5837 - val_acc: 0.8075\n",
      "Epoch 14/20\n",
      "112/112 [==============================] - 510s 5s/step - loss: 0.2923 - acc: 0.8906 - val_loss: 0.7487 - val_acc: 0.7544\n",
      "Epoch 15/20\n",
      "112/112 [==============================] - 509s 5s/step - loss: 0.2815 - acc: 0.9018 - val_loss: 0.5998 - val_acc: 0.8232\n",
      "Epoch 16/20\n",
      "112/112 [==============================] - 510s 5s/step - loss: 0.2913 - acc: 0.8996 - val_loss: 0.7131 - val_acc: 0.7937\n",
      "Epoch 17/20\n",
      "112/112 [==============================] - 512s 5s/step - loss: 0.2747 - acc: 0.9023 - val_loss: 0.7272 - val_acc: 0.7662\n",
      "Epoch 18/20\n",
      "112/112 [==============================] - 508s 5s/step - loss: 0.2354 - acc: 0.9124 - val_loss: 0.5390 - val_acc: 0.8310\n",
      "Epoch 19/20\n",
      "112/112 [==============================] - 511s 5s/step - loss: 0.2705 - acc: 0.9046 - val_loss: 0.8795 - val_acc: 0.7878\n",
      "Epoch 20/20\n",
      "112/112 [==============================] - 510s 5s/step - loss: 0.1828 - acc: 0.9297 - val_loss: 0.6017 - val_acc: 0.8251\n",
      "Found 210 images belonging to 6 classes.\n",
      "CONFUSION MATRIX:\n",
      "[[25  0  1 13  1  0]\n",
      " [ 0 34  2  0  4  0]\n",
      " [ 1  7 29  0  2  1]\n",
      " [ 3  0  1 34  1  1]\n",
      " [ 0 14  3  2 21  0]\n",
      " [ 0  1  0  5  0  4]]\n",
      "CLASSIFICATION REPORT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   cardboard       0.86      0.62      0.72        40\n",
      "       glass       0.61      0.85      0.71        40\n",
      "       metal       0.81      0.72      0.76        40\n",
      "       paper       0.63      0.85      0.72        40\n",
      "     plastic       0.72      0.53      0.61        40\n",
      "       trash       0.67      0.40      0.50        10\n",
      "\n",
      "   micro avg       0.70      0.70      0.70       210\n",
      "   macro avg       0.72      0.66      0.67       210\n",
      "weighted avg       0.72      0.70      0.70       210\n",
      "\n",
      "*************** Test  6  Batch size:  16  Epochs:  30 ***************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1792 images belonging to 6 classes.\n",
      "Found 525 images belonging to 6 classes.\n",
      "Epoch 1/30\n",
      "112/112 [==============================] - 560s 5s/step - loss: 1.6625 - acc: 0.4799 - val_loss: 0.7117 - val_acc: 0.7676\n",
      "Epoch 2/30\n",
      "112/112 [==============================] - 512s 5s/step - loss: 0.9170 - acc: 0.6590 - val_loss: 0.7640 - val_acc: 0.7092\n",
      "Epoch 3/30\n",
      "112/112 [==============================] - 516s 5s/step - loss: 0.7654 - acc: 0.7249 - val_loss: 0.7009 - val_acc: 0.7544\n",
      "Epoch 4/30\n",
      "112/112 [==============================] - 516s 5s/step - loss: 0.6665 - acc: 0.7561 - val_loss: 0.6120 - val_acc: 0.7760\n",
      "Epoch 5/30\n",
      "112/112 [==============================] - 515s 5s/step - loss: 0.5883 - acc: 0.7773 - val_loss: 0.7676 - val_acc: 0.7525\n",
      "Epoch 6/30\n",
      "112/112 [==============================] - 513s 5s/step - loss: 0.5417 - acc: 0.8092 - val_loss: 0.6300 - val_acc: 0.7898\n",
      "Epoch 7/30\n",
      "112/112 [==============================] - 511s 5s/step - loss: 0.4836 - acc: 0.8309 - val_loss: 0.6156 - val_acc: 0.8094\n",
      "Epoch 8/30\n",
      "112/112 [==============================] - 511s 5s/step - loss: 0.4574 - acc: 0.8231 - val_loss: 0.8415 - val_acc: 0.6857\n",
      "Epoch 9/30\n",
      "112/112 [==============================] - 512s 5s/step - loss: 0.4087 - acc: 0.8521 - val_loss: 0.6829 - val_acc: 0.7760\n",
      "Epoch 10/30\n",
      "112/112 [==============================] - 512s 5s/step - loss: 0.3946 - acc: 0.8549 - val_loss: 0.5647 - val_acc: 0.8212\n",
      "Epoch 11/30\n",
      "112/112 [==============================] - 512s 5s/step - loss: 0.3616 - acc: 0.8717 - val_loss: 0.7328 - val_acc: 0.7741\n",
      "Epoch 12/30\n",
      "112/112 [==============================] - 512s 5s/step - loss: 0.3395 - acc: 0.8772 - val_loss: 0.6068 - val_acc: 0.7976\n",
      "Epoch 13/30\n",
      "112/112 [==============================] - 513s 5s/step - loss: 0.2617 - acc: 0.9018 - val_loss: 0.8999 - val_acc: 0.7367\n",
      "Epoch 14/30\n",
      "112/112 [==============================] - 515s 5s/step - loss: 0.3195 - acc: 0.8884 - val_loss: 0.6113 - val_acc: 0.7839\n",
      "Epoch 15/30\n",
      "112/112 [==============================] - 516s 5s/step - loss: 0.2844 - acc: 0.9051 - val_loss: 0.6285 - val_acc: 0.7957\n",
      "Epoch 16/30\n",
      "112/112 [==============================] - 515s 5s/step - loss: 0.2844 - acc: 0.8951 - val_loss: 0.6468 - val_acc: 0.7721\n",
      "Epoch 17/30\n",
      "112/112 [==============================] - 516s 5s/step - loss: 0.3083 - acc: 0.8962 - val_loss: 0.6429 - val_acc: 0.7859\n",
      "Epoch 18/30\n",
      "112/112 [==============================] - 513s 5s/step - loss: 0.2562 - acc: 0.9079 - val_loss: 0.7995 - val_acc: 0.7525\n",
      "Epoch 19/30\n",
      "112/112 [==============================] - 514s 5s/step - loss: 0.2109 - acc: 0.9314 - val_loss: 0.6155 - val_acc: 0.8075\n",
      "Epoch 20/30\n",
      "112/112 [==============================] - 515s 5s/step - loss: 0.2579 - acc: 0.9113 - val_loss: 0.9266 - val_acc: 0.7387\n",
      "Epoch 21/30\n",
      "112/112 [==============================] - 513s 5s/step - loss: 0.2140 - acc: 0.9241 - val_loss: 0.6909 - val_acc: 0.8153\n",
      "Epoch 22/30\n",
      "112/112 [==============================] - 516s 5s/step - loss: 0.2327 - acc: 0.9057 - val_loss: 0.7489 - val_acc: 0.7996\n",
      "Epoch 23/30\n",
      "112/112 [==============================] - 510s 5s/step - loss: 0.2028 - acc: 0.9269 - val_loss: 0.8918 - val_acc: 0.7917\n",
      "Epoch 24/30\n",
      "112/112 [==============================] - 511s 5s/step - loss: 0.1912 - acc: 0.9275 - val_loss: 0.8519 - val_acc: 0.7898\n",
      "Epoch 25/30\n",
      "112/112 [==============================] - 513s 5s/step - loss: 0.1773 - acc: 0.9369 - val_loss: 0.8277 - val_acc: 0.7976\n",
      "Epoch 26/30\n",
      "112/112 [==============================] - 516s 5s/step - loss: 0.2367 - acc: 0.9230 - val_loss: 0.7783 - val_acc: 0.7800\n",
      "Epoch 27/30\n",
      "112/112 [==============================] - 516s 5s/step - loss: 0.2006 - acc: 0.9302 - val_loss: 0.9385 - val_acc: 0.7839\n",
      "Epoch 28/30\n",
      "112/112 [==============================] - 514s 5s/step - loss: 0.1856 - acc: 0.9408 - val_loss: 0.7234 - val_acc: 0.8114\n",
      "Epoch 29/30\n",
      "112/112 [==============================] - 514s 5s/step - loss: 0.1818 - acc: 0.9369 - val_loss: 0.7662 - val_acc: 0.7937\n",
      "Epoch 30/30\n",
      "112/112 [==============================] - 513s 5s/step - loss: 0.1763 - acc: 0.9420 - val_loss: 0.8947 - val_acc: 0.7917\n",
      "Found 210 images belonging to 6 classes.\n",
      "CONFUSION MATRIX:\n",
      "[[28  0  0 12  0  0]\n",
      " [ 1 29  8  0  2  0]\n",
      " [ 0  1 36  1  0  2]\n",
      " [ 1  0  1 36  0  2]\n",
      " [ 0  7 15  4 11  3]\n",
      " [ 0  0  0  3  0  7]]\n",
      "CLASSIFICATION REPORT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   cardboard       0.93      0.70      0.80        40\n",
      "       glass       0.78      0.72      0.75        40\n",
      "       metal       0.60      0.90      0.72        40\n",
      "       paper       0.64      0.90      0.75        40\n",
      "     plastic       0.85      0.28      0.42        40\n",
      "       trash       0.50      0.70      0.58        10\n",
      "\n",
      "   micro avg       0.70      0.70      0.70       210\n",
      "   macro avg       0.72      0.70      0.67       210\n",
      "weighted avg       0.75      0.70      0.68       210\n",
      "\n",
      "*************** Test  7  Batch size:  32  Epochs:  10 ***************\n",
      "Found 1792 images belonging to 6 classes.\n",
      "Found 525 images belonging to 6 classes.\n",
      "Epoch 1/10\n",
      "56/56 [==============================] - 519s 9s/step - loss: 8.7449 - acc: 0.3722 - val_loss: 9.9875 - val_acc: 0.3594\n",
      "Epoch 2/10\n",
      "56/56 [==============================] - 473s 8s/step - loss: 8.9329 - acc: 0.4124 - val_loss: 10.1012 - val_acc: 0.3671\n",
      "Epoch 3/10\n",
      "56/56 [==============================] - 475s 8s/step - loss: 5.7808 - acc: 0.4202 - val_loss: 1.1208 - val_acc: 0.5091\n",
      "Epoch 4/10\n",
      "56/56 [==============================] - 474s 8s/step - loss: 1.0102 - acc: 0.6021 - val_loss: 0.8390 - val_acc: 0.6734\n",
      "Epoch 5/10\n",
      "56/56 [==============================] - 470s 8s/step - loss: 0.7613 - acc: 0.7126 - val_loss: 0.6786 - val_acc: 0.7647\n",
      "Epoch 6/10\n",
      "56/56 [==============================] - 478s 9s/step - loss: 0.7050 - acc: 0.7427 - val_loss: 0.6487 - val_acc: 0.7830\n",
      "Epoch 7/10\n",
      "56/56 [==============================] - 479s 9s/step - loss: 0.5674 - acc: 0.7958 - val_loss: 0.6039 - val_acc: 0.7688\n",
      "Epoch 8/10\n",
      "56/56 [==============================] - 476s 9s/step - loss: 0.5630 - acc: 0.8025 - val_loss: 0.6465 - val_acc: 0.7890\n",
      "Epoch 9/10\n",
      "56/56 [==============================] - 478s 9s/step - loss: 0.4607 - acc: 0.8270 - val_loss: 0.5553 - val_acc: 0.8114\n",
      "Epoch 10/10\n",
      "56/56 [==============================] - 473s 8s/step - loss: 0.3850 - acc: 0.8644 - val_loss: 0.6669 - val_acc: 0.7830\n",
      "Found 210 images belonging to 6 classes.\n",
      "CONFUSION MATRIX:\n",
      "[[33  0  1  4  2  0]\n",
      " [ 0 28  1  0 11  0]\n",
      " [ 0  8 25  0  4  3]\n",
      " [ 7  0  1 28  4  0]\n",
      " [ 0  3  1  0 36  0]\n",
      " [ 1  0  0  1  3  5]]\n",
      "CLASSIFICATION REPORT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   cardboard       0.80      0.82      0.81        40\n",
      "       glass       0.72      0.70      0.71        40\n",
      "       metal       0.86      0.62      0.72        40\n",
      "       paper       0.85      0.70      0.77        40\n",
      "     plastic       0.60      0.90      0.72        40\n",
      "       trash       0.62      0.50      0.56        10\n",
      "\n",
      "   micro avg       0.74      0.74      0.74       210\n",
      "   macro avg       0.74      0.71      0.72       210\n",
      "weighted avg       0.76      0.74      0.74       210\n",
      "\n",
      "*************** Test  8  Batch size:  32  Epochs:  20 ***************\n",
      "Found 1792 images belonging to 6 classes.\n",
      "Found 525 images belonging to 6 classes.\n",
      "Epoch 1/20\n",
      "56/56 [==============================] - 516s 9s/step - loss: 1.7149 - acc: 0.4554 - val_loss: 1.0830 - val_acc: 0.5137\n",
      "Epoch 2/20\n",
      "56/56 [==============================] - 472s 8s/step - loss: 0.9107 - acc: 0.6562 - val_loss: 0.6751 - val_acc: 0.7566\n",
      "Epoch 3/20\n",
      "56/56 [==============================] - 474s 8s/step - loss: 0.7654 - acc: 0.7260 - val_loss: 0.6063 - val_acc: 0.7809\n",
      "Epoch 4/20\n",
      "56/56 [==============================] - 474s 8s/step - loss: 0.6476 - acc: 0.7628 - val_loss: 0.6039 - val_acc: 0.7870\n",
      "Epoch 5/20\n",
      "56/56 [==============================] - 473s 8s/step - loss: 0.5752 - acc: 0.7868 - val_loss: 0.6333 - val_acc: 0.7606\n",
      "Epoch 6/20\n",
      "56/56 [==============================] - 471s 8s/step - loss: 0.4681 - acc: 0.8359 - val_loss: 0.5939 - val_acc: 0.8114\n",
      "Epoch 7/20\n",
      "56/56 [==============================] - 470s 8s/step - loss: 0.5070 - acc: 0.8086 - val_loss: 0.6711 - val_acc: 0.7586\n",
      "Epoch 8/20\n",
      "56/56 [==============================] - 471s 8s/step - loss: 0.4050 - acc: 0.8527 - val_loss: 0.6179 - val_acc: 0.8012\n",
      "Epoch 9/20\n",
      "56/56 [==============================] - 471s 8s/step - loss: 0.4592 - acc: 0.8348 - val_loss: 0.4892 - val_acc: 0.8154\n",
      "Epoch 10/20\n",
      "56/56 [==============================] - 472s 8s/step - loss: 0.3744 - acc: 0.8605 - val_loss: 0.6782 - val_acc: 0.7688\n",
      "Epoch 11/20\n",
      "56/56 [==============================] - 469s 8s/step - loss: 0.3132 - acc: 0.8956 - val_loss: 0.6458 - val_acc: 0.8012\n",
      "Epoch 12/20\n",
      "56/56 [==============================] - 473s 8s/step - loss: 0.3715 - acc: 0.8750 - val_loss: 0.7483 - val_acc: 0.7830\n",
      "Epoch 13/20\n",
      "56/56 [==============================] - 467s 8s/step - loss: 0.2860 - acc: 0.8929 - val_loss: 0.7236 - val_acc: 0.8012\n",
      "Epoch 14/20\n",
      "56/56 [==============================] - 473s 8s/step - loss: 0.3466 - acc: 0.8733 - val_loss: 0.6712 - val_acc: 0.7890\n",
      "Epoch 15/20\n",
      "56/56 [==============================] - 472s 8s/step - loss: 0.2361 - acc: 0.9174 - val_loss: 0.6638 - val_acc: 0.8073\n",
      "Epoch 16/20\n",
      "56/56 [==============================] - 469s 8s/step - loss: 0.3193 - acc: 0.8795 - val_loss: 0.6848 - val_acc: 0.8093\n",
      "Epoch 17/20\n",
      "56/56 [==============================] - 470s 8s/step - loss: 0.2491 - acc: 0.9096 - val_loss: 0.8149 - val_acc: 0.7485\n",
      "Epoch 18/20\n",
      "56/56 [==============================] - 474s 8s/step - loss: 0.2515 - acc: 0.9062 - val_loss: 0.6835 - val_acc: 0.7969\n",
      "Epoch 19/20\n",
      "56/56 [==============================] - 470s 8s/step - loss: 0.1959 - acc: 0.9213 - val_loss: 0.8048 - val_acc: 0.7748\n",
      "Epoch 20/20\n",
      "56/56 [==============================] - 469s 8s/step - loss: 0.2098 - acc: 0.9280 - val_loss: 0.7508 - val_acc: 0.8012\n",
      "Found 210 images belonging to 6 classes.\n",
      "CONFUSION MATRIX:\n",
      "[[32  0  0  7  1  0]\n",
      " [ 1 28  3  0  8  0]\n",
      " [ 1  7 21  3  6  2]\n",
      " [ 1  0  1 36  2  0]\n",
      " [ 0  4  2  1 33  0]\n",
      " [ 1  1  0  4  1  3]]\n",
      "CLASSIFICATION REPORT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   cardboard       0.89      0.80      0.84        40\n",
      "       glass       0.70      0.70      0.70        40\n",
      "       metal       0.78      0.53      0.63        40\n",
      "       paper       0.71      0.90      0.79        40\n",
      "     plastic       0.65      0.82      0.73        40\n",
      "       trash       0.60      0.30      0.40        10\n",
      "\n",
      "   micro avg       0.73      0.73      0.73       210\n",
      "   macro avg       0.72      0.67      0.68       210\n",
      "weighted avg       0.74      0.73      0.72       210\n",
      "\n",
      "*************** Test  9  Batch size:  32  Epochs:  30 ***************\n",
      "Found 1792 images belonging to 6 classes.\n",
      "Found 525 images belonging to 6 classes.\n",
      "Epoch 1/30\n",
      "56/56 [==============================] - 517s 9s/step - loss: 4.4036 - acc: 0.4102 - val_loss: 4.3613 - val_acc: 0.5293\n",
      "Epoch 2/30\n",
      "56/56 [==============================] - 476s 9s/step - loss: 1.7905 - acc: 0.5960 - val_loss: 0.7475 - val_acc: 0.7302\n",
      "Epoch 3/30\n",
      "56/56 [==============================] - 476s 8s/step - loss: 0.8377 - acc: 0.6758 - val_loss: 0.6360 - val_acc: 0.7809\n",
      "Epoch 4/30\n",
      "56/56 [==============================] - 473s 8s/step - loss: 0.7144 - acc: 0.7427 - val_loss: 0.6812 - val_acc: 0.7525\n",
      "Epoch 5/30\n",
      "56/56 [==============================] - 475s 8s/step - loss: 0.5903 - acc: 0.7879 - val_loss: 0.6558 - val_acc: 0.7667\n",
      "Epoch 6/30\n",
      "56/56 [==============================] - 468s 8s/step - loss: 0.5321 - acc: 0.8013 - val_loss: 0.5473 - val_acc: 0.8053\n",
      "Epoch 7/30\n",
      "56/56 [==============================] - 470s 8s/step - loss: 0.4606 - acc: 0.8337 - val_loss: 0.6556 - val_acc: 0.8012\n",
      "Epoch 8/30\n",
      "56/56 [==============================] - 470s 8s/step - loss: 0.4933 - acc: 0.8181 - val_loss: 0.5996 - val_acc: 0.7951\n",
      "Epoch 9/30\n",
      "56/56 [==============================] - 470s 8s/step - loss: 0.3587 - acc: 0.8744 - val_loss: 0.5748 - val_acc: 0.7972\n",
      "Epoch 10/30\n",
      "56/56 [==============================] - 468s 8s/step - loss: 0.3416 - acc: 0.8789 - val_loss: 0.6654 - val_acc: 0.7890\n",
      "Epoch 11/30\n",
      "56/56 [==============================] - 468s 8s/step - loss: 0.3274 - acc: 0.8733 - val_loss: 0.6535 - val_acc: 0.7830\n",
      "Epoch 12/30\n",
      "56/56 [==============================] - 469s 8s/step - loss: 0.3160 - acc: 0.8772 - val_loss: 0.6207 - val_acc: 0.7850\n",
      "Epoch 13/30\n",
      "56/56 [==============================] - 465s 8s/step - loss: 0.3405 - acc: 0.8800 - val_loss: 0.7119 - val_acc: 0.7830\n",
      "Epoch 14/30\n",
      "56/56 [==============================] - 470s 8s/step - loss: 0.2993 - acc: 0.8845 - val_loss: 0.6453 - val_acc: 0.7911\n",
      "Epoch 15/30\n",
      "56/56 [==============================] - 469s 8s/step - loss: 0.2733 - acc: 0.8923 - val_loss: 0.6163 - val_acc: 0.8114\n",
      "Epoch 16/30\n",
      "56/56 [==============================] - 469s 8s/step - loss: 0.2410 - acc: 0.9107 - val_loss: 0.7823 - val_acc: 0.7708\n",
      "Epoch 17/30\n",
      "56/56 [==============================] - 471s 8s/step - loss: 0.2278 - acc: 0.9141 - val_loss: 0.7777 - val_acc: 0.7789\n",
      "Epoch 18/30\n",
      "56/56 [==============================] - 473s 8s/step - loss: 0.2533 - acc: 0.9057 - val_loss: 0.7360 - val_acc: 0.7793\n",
      "Epoch 19/30\n",
      "56/56 [==============================] - 470s 8s/step - loss: 0.2251 - acc: 0.9230 - val_loss: 0.6843 - val_acc: 0.7992\n",
      "Epoch 20/30\n",
      "56/56 [==============================] - 469s 8s/step - loss: 0.2067 - acc: 0.9252 - val_loss: 0.7250 - val_acc: 0.7972\n",
      "Epoch 21/30\n",
      "56/56 [==============================] - 469s 8s/step - loss: 0.1672 - acc: 0.9381 - val_loss: 0.7441 - val_acc: 0.7850\n",
      "Epoch 22/30\n",
      "56/56 [==============================] - 471s 8s/step - loss: 0.2069 - acc: 0.9269 - val_loss: 0.6063 - val_acc: 0.8195\n",
      "Epoch 23/30\n",
      "56/56 [==============================] - 469s 8s/step - loss: 0.1590 - acc: 0.9431 - val_loss: 0.7116 - val_acc: 0.8114\n",
      "Epoch 24/30\n",
      "56/56 [==============================] - 469s 8s/step - loss: 0.1505 - acc: 0.9453 - val_loss: 0.7498 - val_acc: 0.7972\n",
      "Epoch 25/30\n",
      "56/56 [==============================] - 469s 8s/step - loss: 0.1957 - acc: 0.9330 - val_loss: 0.5513 - val_acc: 0.8357\n",
      "Epoch 26/30\n",
      "56/56 [==============================] - 469s 8s/step - loss: 0.1071 - acc: 0.9643 - val_loss: 0.7097 - val_acc: 0.8256\n",
      "Epoch 27/30\n",
      "56/56 [==============================] - 471s 8s/step - loss: 0.1630 - acc: 0.9347 - val_loss: 0.6960 - val_acc: 0.8154\n",
      "Epoch 28/30\n",
      "56/56 [==============================] - 468s 8s/step - loss: 0.1435 - acc: 0.9459 - val_loss: 0.5899 - val_acc: 0.8377\n",
      "Epoch 29/30\n",
      "56/56 [==============================] - 468s 8s/step - loss: 0.1323 - acc: 0.9548 - val_loss: 0.6739 - val_acc: 0.8256\n",
      "Epoch 30/30\n",
      "56/56 [==============================] - 463s 8s/step - loss: 0.1869 - acc: 0.9336 - val_loss: 0.8041 - val_acc: 0.7667\n",
      "Found 210 images belonging to 6 classes.\n",
      "CONFUSION MATRIX:\n",
      "[[31  0  0  9  0  0]\n",
      " [ 0 32  1  0  7  0]\n",
      " [ 0  7 26  2  4  1]\n",
      " [ 2  0  1 36  1  0]\n",
      " [ 0  4  4  2 29  1]\n",
      " [ 1  1  0  3  0  5]]\n",
      "CLASSIFICATION REPORT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   cardboard       0.91      0.78      0.84        40\n",
      "       glass       0.73      0.80      0.76        40\n",
      "       metal       0.81      0.65      0.72        40\n",
      "       paper       0.69      0.90      0.78        40\n",
      "     plastic       0.71      0.72      0.72        40\n",
      "       trash       0.71      0.50      0.59        10\n",
      "\n",
      "   micro avg       0.76      0.76      0.76       210\n",
      "   macro avg       0.76      0.72      0.73       210\n",
      "weighted avg       0.77      0.76      0.76       210\n",
      "\n"
     ]
    }
   ],
   "source": [
    "runExperiments()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "martin",
   "language": "python",
   "name": "martin"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
